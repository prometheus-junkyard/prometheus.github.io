<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="An open-source monitoring system with a dimensional data model, flexible query language, efficient time series database and modern alerting approach.">
    <meta name="keywords" content="prometheus, monitoring, monitoring system, time series, time series database, alerting, metrics, telemetry">
    <meta name="author" content="Prometheus">

    <link rel="alternate" type="application/atom+xml" title="Prometheus Blog » Feed" href="/blog/feed.xml">

    <link rel="shortcut icon" href="/assets/favicons/favicon.ico">
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/favicons/apple-touch-icon-57x57-cbe16921e26.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/favicons/apple-touch-icon-60x60-cb34684d423.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/favicons/apple-touch-icon-72x72-cb804bd5fae.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/favicons/apple-touch-icon-76x76-cb4a7d77a78.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/favicons/apple-touch-icon-114x114-cbb4144a5d4.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/favicons/apple-touch-icon-120x120-cb24b29faf0.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/favicons/apple-touch-icon-144x144-cb590f03c01.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/favicons/apple-touch-icon-152x152-cbb45d875fa.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicons/apple-touch-icon-180x180-cb5da064c37.png">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-32x32-cb6bc898e38.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/assets/favicons/android-chrome-192x192-cbd9b99e56f.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-96x96-cbe466ffcf9.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-16x16-cb02406aa58.png" sizes="16x16">
    <link rel="manifest" href="/assets/favicons/android-chrome-manifest.json">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-TileImage" content="/assets/favicons/mstile-144x144.png">
    <meta name="theme-color" content="#ffffff">

    
    <title>Blog | Prometheus</title>
    

    <!-- Bootstrap core CSS -->
    <link href="/assets/bootstrap-3.3.1/css/bootstrap.min-cb3ab3438f8.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="/css/docs-cb74caebabb.css" rel="stylesheet">
    <link href="/css/routing-tree-editor-cbd4d13cac6.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="/assets/font-awesome-4.2.0/css/font-awesome.min-cbfeda974a7.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Lato:300,300italic,400' rel='stylesheet' type='text/css'>

  </head>

  <body>

  <div class="">
    <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/"><img src="/assets/prometheus_logo_grey.svg" alt="Prometheus logo"> Prometheus</a>
        </div>
        <div class="collapse navbar-collapse" id="navbar">
          <ul class="nav navbar-nav navbar-right main-nav">
            <li><a href="/docs/introduction/overview/">Docs</a></li>
            <li><a href="/download/">Download</a></li>
            <li><a href="/community/">Community</a></li>
            <li><a href="/blog/">Blog</a></li>
            <li><a href="https://github.com/prometheus"><i class="fa fa-github"></i></a></li>
            <li><a href="https://twitter.com/PrometheusIO"><i class="fa fa-twitter"></i></a></li>
          </ul>
        </div>
      </div>
    </nav>
  </div>


<div class="container">
  <div class="row">
  <div class="col-md-9">
    
      <div class="blog doc-content">
        <h1><a href="/blog/2017/09/04/promcon-2017-recap/">PromCon 2017 Recap</a></h1>
        <aside>Posted at: September 4, 2017 by Julius Volz</aside>
        <article class="doc-content">
          <h2 id="what-happened">What happened<a class="header-anchor" href="#what-happened" name="what-happened"></a>
</h2>

<p>Two weeks ago, Prometheus users and developers from all over the world came together in Munich for <a href="https://promcon.io/2017-munich/">PromCon 2017</a>, the second conference around the Prometheus monitoring system. The purpose of this event was to exchange knowledge and best practices and build professional connections around monitoring with Prometheus. Google's Munich office offered us a much larger space this year, which allowed us to grow from 80 to 220 attendees while still selling out!</p>

<p>Take a look at the recap video to get an impression of the event:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/4Pr-z8-r1eo" frameborder="0" allowfullscreen></iframe>

<div class='read-more'><a class='btn btn-primary' href='/blog/2017/09/04/promcon-2017-recap/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2017/06/21/prometheus-20-alpha3-new-rule-format/">Prometheus 2.0 Alpha.3 with New Rule Format</a></h1>
        <aside>Posted at: June 22, 2017 by Goutham Veeramachaneni</aside>
        <article class="doc-content">
          <p>Today we release the third alpha version of Prometheus 2.0. Aside from a variety of bug fixes in the new storage layer, it contains a few planned breaking changes.</p>

<h2 id="flag-changes">Flag Changes<a class="header-anchor" href="#flag-changes" name="flag-changes"></a>
</h2>

<p>First, we moved to a new flag library, which uses the more common double-dash <code>--</code> prefix for flags instead of the single dash Prometheus used so far. Deployments have to be adapted accordingly.
Additionally, some flags were removed with this alpha. The full list since Prometheus 1.0.0 is:</p>

<ul>
<li><code>web.telemetry-path</code></li>
<li>All <code>storage.remote.*</code> flags</li>
<li>All <code>storage.local.*</code> flags</li>
<li><code>query.staleness-delta</code></li>
<li><code>alertmanager.url</code></li>
</ul>

<h2 id="recording-rules-changes">Recording Rules changes<a class="header-anchor" href="#recording-rules-changes" name="recording-rules-changes"></a>
</h2>

<p>Alerting and recording rules are one of the critical features of Prometheus. But they also come with a few design issues and missing features, namely:</p>

<ul>
<li><p>All rules ran with the same interval. We could have some heavy rules that are better off being run at a 10-minute interval and some rules that could be run at 15-second intervals.</p></li>
<li><p>All rules were evaluated concurrently, which is actually Prometheus’ oldest <a href="https://github.com/prometheus/prometheus/blob/master/rules/manager.go#L267">open bug</a>. This has a couple of issues, the obvious one being that the load spikes every eval interval if you have a lot of rules. The other being that rules that depend on each other might be fed outdated data. For example:</p></li>
</ul>

<pre><code>instance:network_bytes:rate1m = sum by(instance) (rate(network_bytes_total[1m]))

ALERT HighNetworkTraffic
  IF instance:network_bytes:rate1m &gt; 10e6
  FOR 5m
</code></pre>

<p>Here we are alerting over <code>instance:network_bytes:rate1m</code>, but <code>instance:network_bytes:rate1m</code> is itself being generated by another rule. We can get expected results only if the alert <code>HighNetworkTraffic</code> is run after the current value for <code>instance:network_bytes:rate1m</code> gets recorded.</p>

<ul>
<li>Rules and alerts required users to learn yet another DSL.</li>
</ul>

<p>To solve the issues above, grouping of rules has been <a href="https://github.com/prometheus/prometheus/issues/1095">proposed long back</a> but has only recently been implemented <a href="https://github.com/prometheus/prometheus/pull/2842">as a part of Prometheus 2.0</a>. As part of this implementation we have also moved the rules to the well-known YAML format, which also makes it easier to generate alerting rules based on common patterns in users’ environments.</p>

<p>Here’s how the new format looks:</p>

<pre><code class="yaml">groups:
- name: my-group-name
  interval: 30s   # defaults to global interval
  rules:
  - record: instance:errors:rate5m
    expr: rate(errors_total[5m])
  - record: instance:requests:rate5m
    expr: rate(requests_total[5m])
  - alert: HighErrors
    # Expressions remain PromQL as before and can be spread over
    # multiple lines via YAML’s multi-line strings.
    expr: |
      sum without(instance) (instance:errors:rate5m)
      / 
      sum without(instance) (instance:requests:rate5m)
    for: 5m
    labels:
      severity: critical
    annotations:
      description: "stuff's happening with {{ $labels.service }}"      
</code></pre>

<p>The rules in each group are executed sequentially and you can have an evaluation interval per group.</p>

<p>As this change is breaking, we are going to release it with the 2.0 release and have added a command to promtool for the migration: <code>promtool update rules &lt;filenames&gt;</code>
The converted files have the <code>.yml</code> suffix appended and the <code>rule_files</code> clause in your Prometheus configuration has to be adapted.</p>

<p>Help us moving towards the Prometheus 2.0 stable release by testing this new alpha version! You can report bugs on our <a href="https://github.com/prometheus/prometheus/issues">issue tracker</a> and provide general feedback via our <a href="https://prometheus.io/community/">community channels</a>.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2017/06/14/interview-with-latelier-animation/">Interview with L’Atelier Animation</a></h1>
        <aside>Posted at: June 14, 2017 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, Philippe Panaite
and Barthelemy Stevens from L’Atelier Animation talk about how they switched
their animation studio from a mix of Nagios, Graphite and InfluxDB to
Prometheus.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-l’atelier-animation-does?">Can you tell us about yourself and what L’Atelier Animation does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-l-atelier-animation-does" name="can-you-tell-us-about-yourself-and-what-l-atelier-animation-does"></a>
</h2>

<p><a href="http://www.latelieranimation.com/">L’Atelier Animation</a> is a 3D animation studio based in
the beautiful city of Montreal Canada. Our first feature film
<a href="http://www.imdb.com/title/tt2261287/combined">"Ballerina"</a> (also known as
"Leap") was released worldwide in 2017, US release is expected later this year.</p>

<p>We’re currently hard at work on an animated TV series and on our second feature
film.
 
Our infrastructure consists of around 300 render blades, 150 workstations and
twenty various servers. With the exception of a couple of Macs, everything runs
on Linux (<a href="https://www.centos.org/">CentOS</a>) and not a single Windows machine.   </p>

<p> </p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p> 
At first we went with a mix of <a href="https://www.nagios.org/">Nagios</a>,
<a href="https://graphiteapp.org/">Graphite</a>, and
<a href="https://www.influxdata.com">InfluxDB</a>.  The initial setup was “ok” but nothing
special and over complicated (too many moving parts).   </p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p> 
When we switched all of our services to CentOS 7, we looked at new monitoring
solutions and Prometheus came up for many reasons, but most importantly:</p>

<ul>
<li>Node Exporter: With its customization capabilities, we can fetch any data from clients</li>
<li>SNMP support: Removes the need for a 3rd party SNMP service</li>
<li>Alerting system: ByeBye Nagios</li>
<li>
<a href="https://grafana.com/">Grafana</a> support</li>
</ul>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>When we finished our first film we had a bit of a downtime so it was a perfect
opportunity for our IT department to make big changes. We decided to flush our
whole monitoring system as it was not as good as we wanted.   </p>

<p>One of the most important part is to monitor networking equipment so we started
by configuring <a href="https://github.com/prometheus/snmp_exporter/">snmp_exporter</a> to
fetch data from one of our switches. The calls to NetSNMP that the exporter
makes are different under CentOS so we had to re-compile some of the binaries,
we did encounter small hiccups here and there but with the help of Brian Brazil
from <a href="https://www.robustperception.io/">Robust Perception</a>, we got everything
sorted out quickly. Once we got snmp_exporter working, we were able to easily
add new devices and fetch SNMP data. We now have our core network monitored in
Grafana (including 13 switches, 10 VLANs).</p>

<p><img src="/assets/blog/2017-06-14/switches-cb08d341048.png" alt="Switch metrics from SNMP data"></p>

<p>After that we configured
<a href="https://github.com/prometheus/node_exporter/">node_exporter</a> as we required
analytics on workstations, render blades and servers. In our field, when a CPU
is not at 100% it’s a problem, we want to use all the power we can so in the
end temperature is more critical. Plus, we need as much uptime as possible so
all our stations have email alerts setup via Prometheus’s
<a href="https://prometheus.io/docs/alerting/alertmanager/">Alertmanager</a> so we’re
aware when anything is down.</p>

<p><img src="/assets/blog/2017-06-14/workstation-cbc6873a556.png" alt="Dashboard for one workstation"></p>

<p>Our specific needs require us to monitor custom data from clients, it’s made
easy through the use of node_exporter’s <a href="https://github.com/prometheus/node_exporter#textfile-collector">textfile
collector</a>
function. A cronjob outputs specific data from any given tool into a
pre-formatted text file in a format readable by Prometheus.   </p>

<p>Since all the data is available through the HTTP protocol, we wrote a
<a href="https://www.python.org/">Python</a> script to fetch data from Prometheus. We
store it in a <a href="https://www.mysql.com/">MySQL</a> database accessed via a web
application that creates a live floor map. This allows us to know with a simple
mouse over which user is seated where with what type of hardware.  We also
created another page with user’s picture &amp; departement information, it helps
new employees know who’s their neighbour.  The website is still an ongoing
project so please don’t judge the look, we’re sysadmins after all not web
designers :-)</p>

<p><img src="/assets/blog/2017-06-14/floormap-cb16108863e.png" alt="Floormap with workstation detail"></p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>It gave us an opportunity to change the way we monitor everything in the studio
and inspired us to create a new custom floor map with all the data which has
been initially fetched by Prometheus. The setup is a lot simpler with one
service to rule them all.</p>

<h2 id="what-do-you-think-the-future-holds-for-l’atelier-animation-and-prometheus?">What do you think the future holds for L’Atelier Animation and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-l-atelier-animation-and-prometheus" name="what-do-you-think-the-future-holds-for-l-atelier-animation-and-prometheus"></a>
</h2>

<p>We’re currently in the process of integrating software licenses usage with
Prometheus. The information will give artists a good idea of whom is using what
and where.</p>

<p>We will continue to customize and add new stuff to Prometheus by user demand
and since we work with artists, we know there will be plenty :-) With SNMP and
the node_exporter’s custom text file inputs, the possibilities are endless...</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2017/05/17/interview-with-iadvize/">Interview with iAdvize</a></h1>
        <aside>Posted at: May 17, 2017 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, Laurent
COMMARIEU from iAdvize talks about how they replaced their legacy Nagios and
Centreon monitoring with Prometheus.</em></p>

<h2 id="can-you-tell-us-about-iadvize-does?">Can you tell us about iAdvize does?<a class="header-anchor" href="#can-you-tell-us-about-iadvize-does" name="can-you-tell-us-about-iadvize-does"></a>
</h2>

<p>I am Laurent COMMARIEU, a system engineer at iAdvize.  I work within the 60
person R&amp;D department in a team of 5 system engineers. Our job is mainly to
ensure that applications, services and the underlying system are up and
running. We are working with developers to ensure the easiest path for their
code to production, and provide the necessary feedback at every step. That’s
where monitoring is important.</p>

<p>iAdvize is a full stack conversational commerce platform.  We provide an easy
way for a brand to centrally interact with their customers, no matter the
communication channel (chat, call, video, Facebook Pages, Facebook Messenger,
Twitter, Instagram, WhatsApp, SMS, etc...). Our customers work in <a href="http://www.iadvize.com/en/customers/">ecommerce,
banks, travel, fashion, etc. in 40
countries</a>. We are an international
company of 200 employees with offices in France, UK, Germany, Spain and Italy.
We raised $16 Million in 2015.</p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>I joined iAdvize in February 2016. Previously I worked in companies specialized
in network and application monitoring. We were working with opensource software
like <a href="https://www.nagios.org/">Nagios</a>, <a href="http://www.cacti.net/">Cacti</a>,
<a href="https://www.centreon.com/">Centreon</a>, <a href="http://www.zabbix.com/">Zabbix</a>,
<a href="https://www.opennms.org/en">OpenNMS</a>, etc. and some non-free ones like <a href="https://saas.hpe.com/en-us/software/network-node-manager-i-network-management-software">HP
NNM</a>,
<a href="http://www-03.ibm.com/software/products/en/netcool-network-management">IBM Netcool
suite</a>,
<a href="http://www.bmc.com/it-solutions/brands/patrol-proactivenet.html">BMC Patrol</a>,
etc.</p>

<p>iAdvize used to delegate monitoring to an external provider. They ensured 24/7
monitoring using Nagios and Centreon. This toolset was working fine with the
legacy static architecture (barebone servers, no VMs, no containers). To
complete this monitoring stack, we also use <a href="https://www.pingdom.com/">Pingdom</a>.</p>

<p>With the moving our monolithic application towards a Microservices architecture
(using Docker) and our will to move our current workload to an infrastructure
cloud provider we needed to have more control and flexibility on monitoring. At
the same time, iAdvize recruited 3 people, which grew the infrastructure team
from 2 to 5.  With the old system it took at least a few days or a week to add
some new metrics into Centreon and had a real cost (time and money).</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>We knew Nagios and the like were not a good choice. Prometheus was the rising
star at the time and we decided to PoC it. <a href="https://sensuapp.org/">Sensu</a> was
also on the list at the beginning but Prometheus seemed more promising for our
use cases.</p>

<p>We needed something able to integrate with Consul, our service discovery
system.  Our micro services already had a /health route; adding a /metrics
endpoint was simple. For about every tool we used, an exporter was available
(MySQL, Memcached, Redis, nginx, FPM, etc.).</p>

<p>On paper it looked good.</p>

<p><img src="/assets/blog/2017-05-17/iadvize-dashboard-1-cbbbcee41b3.png" alt="One of iAdvize's Grafana dashboards"></p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>First of all, we had to convince the developers team (40 people) that
Prometheus was the right tool for the job and that they had to add an exporter
to their apps. So we did a little demo on RabbitMQ, we installed a RabbitMQ
exporter and built a simple <a href="https://grafana.com/">Grafana</a> dashboard to
display usage metrics to developers. A Python script was written to create some
queue and publish/consume messages.</p>

<p>They were quite impressed to see queues and the messages appear in real time.
Before that, developers didn't have access to any monitoring data. Centreon was
restricted by our infrastructure provider. Today, Grafana is available to
everyone at iAdvize, using the Google Auth integration to authenticate. There
are 78 active accounts on it (from dev teams to the CEO).</p>

<p>After we started monitoring existing services with Consul and cAdvisor, we
monitored the actual presence of the containers. They were monitored using
Pingdom checks but it wasn't enough.</p>

<p>We developed a few custom exporters in Go to scrape some business metrics from
our databases (MySQL and Redis).</p>

<p>Soon enough, we were able to replace all the legacy monitoring by Prometheus. </p>

<p><img src="/assets/blog/2017-05-17/iadvize-dashboard-2-cb6d67f0f29.png" alt="One of iAdvize's Grafana dashboards"></p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>Business metrics became very popular and during sales periods everyone is
connected to Grafana to see if we're gonna beat some record.  We monitor the
number of simultaneous conversations, routing errors, agents connected, the
number of visitors loading the iAdvize tag, calls on our API gateway, etc.</p>

<p>We worked for a month to optimize our MySQL servers with analysis based on the
<a href="https://github.com/jfindley/newrelic_exporter">Newrelic exporter</a> and <a href="https://github.com/percona/grafana-dashboards">Percona
dashboard for grafana</a>. It was
a real success, allowing us to discover inefficiencies and perform
optimisations that cut database size by 45% and peak latency by 75%.</p>

<p>There are a lot to say. We know if a AMQP queue has no consumer or if it is
Filling abnormally. We know when a container restarts.</p>

<p>The visibility is just awesome.</p>

<p>That was just for the legacy platform.</p>

<p>More and more micro services are going to be deployed in the cloud and
Prometheus is used to monitor them. We are using Consul to register the
services and Prometheus to discover the metrics routes. Everything works like a
charm and we are able to build a Grafana dashboard with a lot of critical
business, application and system metrics.</p>

<p>We are building a scalable architecture to deploy our services with
<a href="https://www.nomadproject.io/">Nomad</a>. Nomad registers healthy services in
Consul and with some tags relabeling we are able to filter those with a tag
name "metrics=true". It offers to us a huge gain in time to deploy the
monitoring. We have nothing to do ^^.</p>

<p>We also use the EC2 service discovery. It's really useful with auto-scaling
groups. We scale and recycle instances and it's already monitored. No more
waiting for our external infrastructure provider to notice what happens in
production.</p>

<p>We use alertmanager to send some alerts by SMS or in to our
<a href="https://www.flowdock.com/">Flowdock</a>.</p>

<h2 id="what-do-you-think-the-future-holds-for-iadvize-and-prometheus?">What do you think the future holds for iAdvize and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-iadvize-and-prometheus" name="what-do-you-think-the-future-holds-for-iadvize-and-prometheus"></a>
</h2>

<ul>
<li>We are waiting for a simple way to add a long term scalable storage for our
capacity planning.</li>
<li>We have a dream that one day, our auto-scaling will be triggered by
Prometheus alerting. We want to build an autonomous system base on response
time and business metrics.</li>
<li>I used to work with <a href="http://www.netuitive.com/">Netuitive</a>, it had a great
anomaly detection feature with automatic correlation. It would be great to
have some in Prometheus. </li>
</ul>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2017/04/10/promehteus-20-sneak-peak/">Sneak Peak of Prometheus 2.0</a></h1>
        <aside>Posted at: April 10, 2017 by Fabian Reinartz</aside>
        <article class="doc-content">
          <p>In July 2016 Prometheus reached a big milestone with its 1.0 release. Since then, plenty of new features like new service discovery integrations and our experimental remote APIs have been added.
We also realized that new developments in the infrastructure space, in particular <a href="https://kubernetes.io">Kubernetes</a>, allowed monitored environments to become significantly more dynamic. Unsurprisingly, this also brings new challenges to Prometheus and we identified performance bottlenecks in its storage layer.</p>

<p>Over the past few months we have been designing and implementing a new storage concept that addresses those bottlenecks and shows considerable performance improvements overall. It also paves the way to add features such as hot backups.</p>

<p>The changes are so fundamental that it will trigger a new major release: Prometheus 2.0.<br>
Important features and changes beyond the storage are planned before its stable release. However, today we are releasing an early alpha of Prometheus 2.0 to kick off the stabilization process of the new storage.</p>

<p><a href="https://github.com/prometheus/prometheus/releases/tag/v2.0.0-alpha.0">Release tarballs</a> and <a href="https://quay.io/repository/prometheus/prometheus?tab=tags">Docker containers</a> are now available. 
If you are interested in the new mechanics of the storage, make sure to read <a href="https://fabxc.org/blog/2017-04-10-writing-a-tsdb/">the deep-dive blog post</a> looking under the hood.</p>

<p>This version does not work with old storage data and should not replace existing production deployments. To run it, the data directory must be empty and all existing storage flags except for <code>-storage.local.retention</code> have to be removed.</p>

<p>For example; before:</p>

<pre><code>./prometheus -storage.local.retention=200h -storage.local.memory-chunks=1000000 -storage.local.max-chunks-to-persist=500000 -storage.local.chunk-encoding=2 -config.file=/etc/prometheus.yaml
</code></pre>

<p>after:</p>

<pre><code>./prometheus -storage.local.retention=200h -config.file=/etc/prometheus.yaml
</code></pre>

<p>This is a very early version and crashes, data corruption, and bugs in general should be expected. Help us move towards a stable release by submitting them to <a href="https://github.com/prometheus/prometheus/issues">our issue tracker</a>.  </p>

<p>The experimental remote storage APIs are disabled in this alpha release. Scraping targets exposing timestamps, such as federated Prometheus servers, does not yet work. The storage format is breaking and will break again between subsequent alpha releases. We plan to document an upgrade path from 1.0 to 2.0 once we are approaching a stable release.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2017/04/06/interview-with-europace/">Interview with Europace</a></h1>
        <aside>Posted at: April 6, 2017 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, Tobias Gesellchen from
Europace talks about how they discovered Prometheus.</em></p>

<h2 id="can-you-tell-us-about-europace-does?">Can you tell us about Europace does?<a class="header-anchor" href="#can-you-tell-us-about-europace-does" name="can-you-tell-us-about-europace-does"></a>
</h2>

<p><a href="https://www.europace.de/">Europace AG</a> develops and operates the web-based
EUROPACE financial marketplace, which is Germany’s largest platform for
mortgages, building finance products and personal loans. A fully integrated
system links about 400 partners – banks, insurers and financial product
distributors. Several thousand users execute some 35,000 transactions worth a
total of up to €4 billion on EUROPACE every month.  Our engineers regularly
blog at <a href="http://tech.europace.de/">http://tech.europace.de/</a> and
<a href="https://twitter.com/europacetech">@EuropaceTech</a>.</p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p><a href="https://www.nagios.org/">Nagios</a>/<a href="https://www.icinga.com/">Icinga</a> are still
in use for other projects, but with the growing number of services and higher
demand for flexibility we looked for other solutions. Due to Nagios and Icinga
being more centrally maintained, Prometheus matched our aim to have the full
DevOps stack in our team and move specific responsibilities from our
infrastructure team to the project members.</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>Through our activities in the <a href="https://www.meetup.com/Docker-Berlin/">Docker Berlin
community</a> we had been in contact with
<a href="https://soundcloud.com/">SoundCloud</a> and <a href="https://twitter.com/juliusvolz">Julius
Volz</a>, who gave us a good overview. The
combination of flexible Docker containers with the highly flexible label-based
concept convinced us give Prometheus a try.  The Prometheus setup was easy
enough, and the Alertmanager worked for our needs, so that we didn’t see any
reason to try alternatives. Even our little pull requests to improve the
integration in a Docker environment and with messaging tools had been merged
very quickly.  Over time, we added several exporters and Grafana to the stack.
We never looked back or searched for alternatives.</p>

<p><img src="/assets/blog/2017-04-06/europace_grafana_1-cbbe3751886.png" alt="Grafana dashboard for Docker Registry"></p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>Our team introduced Prometheus in a new project, so the transition didn’t
happen in our team. Other teams started by adding Prometheus side by side to
existing solutions and then migrated the metrics collectors step by step.
Custom exporters and other temporary services helped during the migration.
Grafana existed already, so we didn’t have to consider another dashboard. Some
projects still use both Icinga and Prometheus in parallel.</p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>We had issues using Icinga due to scalability - several teams maintaining a
centrally managed solution didn’t work well. Using the Prometheus stack along
with the Alertmanager decoupled our teams and projects.  The Alertmanager is
now able to be deployed in a <a href="https://github.com/prometheus/alertmanager#high-availability">high availability
mode</a>, which is a
great improvement to the heart of our monitoring infrastructure.</p>

<h2 id="what-do-you-think-the-future-holds-for-europace-and-prometheus?">What do you think the future holds for Europace and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-europace-and-prometheus" name="what-do-you-think-the-future-holds-for-europace-and-prometheus"></a>
</h2>

<p>Other teams in our company have gradually adopted Prometheus in their projects.
We expect that more projects will introduce Prometheus along with the
Alertmanager and slowly replace Icinga. With the inherent flexibility of
Prometheus we expect that it will scale with our needs and that we won’t have
issues adapting it to future requirements.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2017/02/20/interview-with-weaveworks/">Interview with Weaveworks</a></h1>
        <aside>Posted at: February 20, 2017 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, Tom Wilkie from
Weaveworks talks about how they choose Prometheus and are now building on it.</em></p>

<h2 id="can-you-tell-us-about-weaveworks?">Can you tell us about Weaveworks?<a class="header-anchor" href="#can-you-tell-us-about-weaveworks" name="can-you-tell-us-about-weaveworks"></a>
</h2>

<p><a href="https://www.weave.works/">Weaveworks</a> offers <a href="https://www.weave.works/solution/cloud/">Weave
Cloud</a>, a service which
"operationalizes" microservices through a combination of open source projects
and software as a service.</p>

<p>Weave Cloud consists of: </p>

<ul>
<li>Visualisation with <a href="https://github.com/weaveworks/scope">Weave Scope</a>
</li>
<li>Continuous Deployment with <a href="https://github.com/weaveworks/flux">Weave Flux</a> </li>
<li>Networking with <a href="https://github.com/weaveworks/weave">Weave Net</a>, the container SDN </li>
<li>
<a href="https://www.weave.works/guides/cloud-guide-part-3-monitor-prometheus-monitoring/">Monitoring with Weave Cortex</a>, our open source, distributed Prometheus-as-a-Service.</li>
</ul>

<p>You can try Weave Cloud <a href="https://cloud.weave.works/signup">free for 60 days</a>.
For the latest on our products check out our <a href="https://www.weave.works/blog/">blog</a>, <a href="https://twitter.com/weaveworks">Twitter</a>, or <a href="https://weave-community.slack.com/">Slack</a> (<a href="https://weaveworks.github.io/community-slack/">invite</a>).</p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>Weave Cloud was a clean-slate implementation, and as such there was no previous
monitoring system. In previous lives the team had used the typical tools such
as Munin and Nagios. Weave Cloud started life as a multitenant, hosted
version of Scope. Scope includes basic monitoring for things like CPU and
memory usage, so I guess you could say we used that. But we needed something
to monitor Scope itself...</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>We've got a bunch of ex-Google SRE on staff, so there was plenty of experience
with Borgmon, and an ex-SoundClouder with experience of Prometheus. We built
the service on Kubernetes and were looking for something that would "fit" with
its dynamically scheduled nature - so Prometheus was a no-brainer. We've even
written a series of blog posts of which <a href="https://www.weave.works/prometheus-kubernetes-perfect-match/">why Prometheus and Kubernetes work together
so well</a> is the first.</p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>When we started with Prometheus the Kubernetes service discovery was still just
a PR and as such there were few docs. We ran a custom build for a while and
kinda just muddled along, working it out for ourselves. Eventually we gave a
talk at the <a href="https://www.meetup.com/Prometheus-London/">London Prometheus meetup</a> on <a href="http://www.slideshare.net/weaveworks/kubernetes-and-prometheus">our experience</a> and published a
<a href="https://www.weave.works/prometheus-kubernetes-deploying/">series</a> of <a href="https://www.weave.works/prometheus-and-kubernetes-monitoring-your-applications/">blog</a> <a href="https://www.weave.works/monitoring-kubernetes-infrastructure/">posts</a>. </p>

<p>We've tried pretty much every different option for running Prometheus. We
started off building our own container images with embedded config, running
them all together in a single Pod alongside Grafana and Alert Manager. We used
ephemeral, in-Pod storage for time series data. We then broke this up into
different Pods so we didn't have to restart Prometheus (and lose history)
whenever we changed our dashboards. More recently we've moved to using
upstream images and storing the config in a Kubernetes config map - which gets
updated by our CI system whenever we change it. We use a small sidecar
container in the Prometheus Pod to watch the config file and ping Prometheus
when it changes. This means we don't have to restart Prometheus very often,
can get away without doing anything fancy for storage, and don't lose history.</p>

<p>Still the problem of periodically losing Prometheus history haunted us, and the
available solutions such as Kubernetes volumes or periodic S3 backups all had
their downsides. Along with our fantastic experience using Prometheus to
monitor the Scope service, this motivated us to build a cloud-native,
distributed version of Prometheus - one which could be upgraded, shuffled
around and survive host failures without losing history. And that’s how Weave
Cortex was born.</p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>Ignoring Cortex for a second, we were particularly excited to see the
introduction of the HA Alert Manager; although mainly because it was one of the
<a href="https://www.weave.works/weave-mesh-prometheus-alertmanager/">first non-Weaveworks projects to use Weave Mesh</a>, 
our gossip and coordination layer.</p>

<p>I was also particularly keen on the version two Kubernetes service discovery
changes by Fabian - this solved an acute problem we were having with monitoring
our Consul Pods, where we needed to scrape multiple ports on the same Pod.</p>

<p>And I'd be remiss if I didn't mention the remote write feature (something I
worked on myself). With this, Prometheus forms a key component of Weave Cortex
itself, scraping targets and sending samples to us.</p>

<h2 id="what-do-you-think-the-future-holds-for-weaveworks-and-prometheus?">What do you think the future holds for Weaveworks and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-weaveworks-and-prometheus" name="what-do-you-think-the-future-holds-for-weaveworks-and-prometheus"></a>
</h2>

<p>For me the immediate future is Weave Cortex, Weaveworks' Prometheus as a
Service. We use it extensively internally, and are starting to achieve pretty
good query performance out of it. It's running in production with real users
right now, and shortly we'll be introducing support for alerting and achieve
feature parity with upstream Prometheus. From there we'll enter a beta
programme of stabilization before general availability in the middle of the
year.</p>

<p>As part of Cortex, we've developed an intelligent Prometheus expression
browser, with autocompletion for PromQL and Jupyter-esque notebooks. We're
looking forward to getting this in front of more people and eventually open
sourcing it.</p>

<p>I've also got a little side project called
<a href="https://github.com/weaveworks-experiments/loki">Loki</a>, which brings Prometheus
service discovery and scraping to OpenTracing, and makes distributed tracing
easy and robust. I'll be giving a <a href="https://cloudnativeeu2017.sched.com/event/9Tbt/loki-an-opensource-zipkin-prometheus-mashup-written-in-go-tom-wilkie-software-engineer">talk about this at KubeCon/CNCFCon
Berlin</a>
at the end of March.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/11/16/interview-with-canonical/">Interview with Canonical</a></h1>
        <aside>Posted at: November 16, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, Canonical talks
about how they are transitioning to Prometheus.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-canonical-does?">Can you tell us about yourself and what Canonical does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-canonical-does" name="can-you-tell-us-about-yourself-and-what-canonical-does"></a>
</h2>

<p><a href="http://www.canonical.com/">Canonical</a> is probably best known as the company
that sponsors Ubuntu Linux.  We also produce or contribute to a number of other
open-source projects including MAAS, Juju, and OpenStack, and provide
commercial support for these products.  Ubuntu powers the majority of OpenStack
deployments, with 55% of production clouds and <a href="https://www.openstack.org/assets/survey/April-2016-User-Survey-Report.pdf#page=47">58% of large cloud
deployments</a>.</p>

<p>My group, BootStack, is our fully managed private cloud service.  We build and
operate OpenStack clouds for Canonical customers.</p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>We’d used a combination of <a href="https://www.nagios.org/">Nagios</a>,
<a href="https://graphite.readthedocs.io/en/latest/">Graphite</a>/<a href="https://github.com/etsy/statsd">statsd</a>,
and in-house <a href="https://www.djangoproject.com/">Django</a> apps. These did not offer
us the level of flexibility and reporting that we need in both our internal and
customer cloud environments.</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>We’d evaluated a few alternatives, including
<a href="https://github.com/influxdata/influxdb">InfluxDB</a> and extending our use of
Graphite, but our first experiences with Prometheus proved it to have the
combination of simplicity and power that we were looking for.  We especially
appreciate the convenience of labels, the simple HTTP protocol, and the out of
box <a href="https://prometheus.io/docs/alerting/rules/">timeseries alerting</a>. The
potential with Prometheus to replace 2 different tools (alerting and trending)
with one is particularly appealing.</p>

<p>Also, several of our staff have prior experience with Borgmon from their time
at Google which greatly added to our interest!</p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>We are still in the process of transitioning, we expect this will take some
time due to the number of custom checks we currently use in our existing
systems that will need to be re-implemented in Prometheus.  The most useful
resource has been the <a href="https://prometheus.io/">prometheus.io</a> site documentation.</p>

<p>It took us a while to choose an exporter.  We originally went with
<a href="https://collectd.org/">collectd</a> but ran into limitations with this.  We’re
working on writing an
<a href="https://github.com/CanonicalLtd/prometheus-openstack-exporter">openstack-exporter</a>
now and were a bit surprised to find there is no good, working, example how to
write exporter from scratch.</p>

<p>Some challenges we’ve run into are: No downsampling support, no long term
storage solution (yet), and we were surprised by the default 2 week retention
period. There's currently no tie-in with Juju, but <a href="https://launchpad.net/prometheus-registration">we’re working on it</a>!</p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>Once we got the hang of exporters, we found they were very easy to write and
have given us very useful metrics.  For example we are developing an
openstack-exporter for our cloud environments.  We’ve also seen very quick
cross-team adoption from our DevOps and WebOps groups and developers.  We don’t
yet have alerting in place but expect to see a lot more to come once we get to
this phase of the transition.</p>

<h2 id="what-do-you-think-the-future-holds-for-canonical-and-prometheus?">What do you think the future holds for Canonical and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-canonical-and-prometheus" name="what-do-you-think-the-future-holds-for-canonical-and-prometheus"></a>
</h2>

<p>We expect Prometheus to be a significant part of our monitoring and reporting
infrastructure, providing the metrics gathering and storage for numerous
current and future systems. We see it potentially replacing Nagios as for
alerting.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/10/12/interview-with-justwatch/">Interview with JustWatch</a></h1>
        <aside>Posted at: October 12, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, JustWatch talks
about how they established their monitoring.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-justwatch-does?">Can you tell us about yourself and what JustWatch does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-justwatch-does" name="can-you-tell-us-about-yourself-and-what-justwatch-does"></a>
</h2>

<p>For consumers, <a href="https://www.justwatch.com">JustWatch</a> is a streaming search
engine that helps to find out where to watch movies and TV shows legally online
and in theaters. You can search movie content across all major streaming
providers like Netflix, HBO, Amazon Video, iTunes, Google Play, and many others
in 17 countries.</p>

<p>For our clients like movie studios or Video on Demand providers, we are an
international movie marketing company that collects anonymized data about
purchase behavior and movie taste of fans worldwide from our consumer apps. We
help studios to advertise their content to the right audience and make digital
video advertising a lot more efficient in minimizing waste coverage.</p>

<p><img src="/assets/blog/2016-10-12/JW_logo_long_black-cb56076c127.jpg" alt="JustWatch logo"></p>

<p>Since our launch in 2014 we went from zero to one of the largest 20k websites
internationally without spending a single dollar on marketing - becoming the
largest streaming search engine worldwide in under two years. Currently, with
an engineering team of just 10, we build and operate a fully dockerized stack
of about 50 micro- and macro-services, running mostly on
<a href="https://kubernetes.io">Kubernetes</a>.</p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>At prior companies many of us worked with most of the open-source monitoring
products there are. We have quite some experience working with
<a href="https://www.nagios.org/">Nagios</a>, <a href="https://www.icinga.org/">Icinga</a>,
<a href="http://www.zabbix.com/">Zabbix</a>,
<a href="https://mmonit.com/monit/documentation/">Monit</a>,
<a href="http://munin-monitoring.org/">Munin</a>, <a href="https://graphiteapp.org/">Graphite</a> and
a few other systems. At one company I helped build a distributed Nagios setup
with Puppet. This setup was nice, since new services automatically showed up in
the system, but taking instances out was still painful. As soon as you have
some variance in your systems, the host and service based monitoring suites
just don’t fit quite well. The label-based approach Prometheus took was
something I always wanted to have, but didn’t find before.</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>At JustWatch the public Prometheus announcement hit exactly the right time. We
mostly had blackbox monitoring for the first few months of the company -
<a href="https://aws.amazon.com/cloudwatch/">CloudWatch</a> for some of the most important
internal metrics, combined with a external services like
<a href="https://www.pingdom.com/">Pingdom</a> for detecting site-wide outages. Also, none
of the classical host-based solutions satisfied us. In a world of containers
and microservices, host-based tools like Icinga,
<a href="https://www.thruk.org/">Thruk</a> or Zabbix felt antiquated and not ready for the
job. When we started to investigate whitebox monitoring, some of us luckily
attended the Golang Meetup where Julius and Björn announced Prometheus. We
quickly set up a Prometheus server and started to instrument our Go services
(we use almost only Go for the backend). It was amazing how easy that was - the
design felt like being cloud- and service-oriented as a first principle and
never got in the way.</p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>Transitioning wasn't that hard, as timing wise, we were lucky enough to go from
no relevant monitoring directly to Prometheus.</p>

<p>The transition to Prometheus was mostly including the Go client into our apps
and wrapping the HTTP handlers. We also wrote and deployed several exporters,
including the <a href="https://github.com/prometheus/node_exporter">node_exporter</a> and
several exporters for cloud provider APIs. In our experience monitoring and
alerting is a project that is never finished, but the bulk of the work was done
within a few weeks as a side project.</p>

<p>Since the deployment of Prometheus we tend to look into metrics whenever we
miss something or when we are designing new services from scratch.</p>

<p>It took some time to fully grasp the elegance of PromQL and labels concept
fully, but the effort really paid off.</p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>Prometheus enlightened us by making it incredibly easy to reap the benefits
from whitebox monitoring and label-based canary deployments. The out-of-the-box
metrics for many Golang aspects (HTTP Handler, Go Runtime) helped us to get to
a return on investment very quickly - goroutine metrics alone saved the day
multiple times. The only monitoring component we actually liked before -
<a href="http://grafana.org/">Grafana</a> - feels like a natural fit for Prometheus and
has allowed us to create some very helpful dashboards. We appreciated that
Prometheus didn't try to reinvent the wheel but rather fit in perfectly with
the best solution out there. Another huge improvement on predecessors was
Prometheus's focus on actually getting the math right (percentiles, etc.). In
other systems, we were never quite sure if the operations offered made sense.
Especially percentiles are such a natural and necessary way of reasoning about
microservice performance that it felt great that they get first class
treatment.</p>

<p><img src="/assets/blog/2016-10-12/prometheus-dashboard-db-cbc4db3f322.jpg" alt="Database Dashboard"></p>

<p>The integrated service discovery makes it super easy to manage the scrape
targets. For Kubernetes, everything just works out-of-the-box. For some other
systems not running on Kubernetes yet, we use a
<a href="https://www.consul.io/">Consul-based</a> approach. All it takes to get an
application monitored by Prometheus is to add the client, expose <code>/metrics</code> and
set one simple annotation on the Container/Pod. This low coupling takes out a
lot of friction between development and operations - a lot of services are
built well orchestrated from the beginning, because it's simple and fun.</p>

<p>The combination of time-series and clever functions make for awesome alerting
super-powers. Aggregations that run on the server and treating both
time-series, combinations of them and even functions on those combinations as
first-class citizens makes alerting a breeze - often times after the fact.</p>

<h2 id="what-do-you-think-the-future-holds-for-justwatch-and-prometheus?">What do you think the future holds for JustWatch and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-justwatch-and-prometheus" name="what-do-you-think-the-future-holds-for-justwatch-and-prometheus"></a>
</h2>

<p>While we value very much that Prometheus doesn't focus on being shiny but on
actually working and delivering value while being reasonably easy to deploy and
operate - especially the Alertmanager leaves a lot to be desired yet. Just some
simple improvements like simplified interactive alert building and editing in
the frontend would go a long way in working with alerts being even simpler.</p>

<p>We are really looking forward to the ongoing improvements in the storage layer,
including remote storage. We also hope for some of the approaches taken in
<a href="https://github.com/weaveworks/prism">Project Prism</a> and
<a href="https://github.com/digitalocean/vulcan">Vulcan</a> to be backported to core
Prometheus. The most interesting topics for us right now are GCE Service
Discovery, easier scaling, and much longer retention periods (even at the cost
of colder storage and much longer query times for older events).</p>

<p>We are also looking forward to use Prometheus for more non-technical
departments as well. We’d like to cover most of our KPIs with Prometheus to
allow everyone to create beautiful dashboards, as well as alerts. We're
currently even planning to abuse the awesome alert engine for a new, internal
business project as well - stay tuned!</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/09/21/interview-with-compose/">Interview with Compose</a></h1>
        <aside>Posted at: September 21, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, Compose talks
about their monitoring journey from Graphite and InfluxDB to Prometheus.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-compose-does?">Can you tell us about yourself and what Compose does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-compose-does" name="can-you-tell-us-about-yourself-and-what-compose-does"></a>
</h2>

<p><a href="https://www.compose.com/">Compose</a> delivers production-ready database clusters
as a service to developers around the world. An app developer can come to us
and in a few clicks have a multi-host, highly available, automatically backed
up and secure database ready in minutes. Those database deployments then
autoscale up as demand increases so a developer can spend their time on
building their great apps, not on running their database.</p>

<p>We have tens of clusters of hosts across at least two regions in each of AWS,
Google Cloud Platform and SoftLayer. Each cluster spans availability zones
where supported and is home to around 1000 highly-available database
deployments in their own private networks. More regions and providers are in
the works.</p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>Before Prometheus, a number of different metrics systems were tried. The first
system we tried was <a href="https://graphiteapp.org/">Graphite</a>, which worked pretty
well initially, but the sheer volume of different metrics we had to store,
combined with the way Whisper files are stored and accessed on disk, quickly
overloaded our systems. While we were aware that Graphite could be scaled
horizontally relatively easily, it would have been an expensive cluster.
<a href="https://www.influxdata.com/">InfluxDB</a> looked more promising so we started
trying out the early-ish versions of that and it seemed to work well for a good
while. Goodbye Graphite. </p>

<p>The earlier versions of InfluxDB had some issues with data corruption
occasionally. We semi-regularly had to purge all of our metrics. It wasn’t a
devastating loss for us normally, but it was irritating. The continued promises
of features that never materialised frankly wore on us.</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>It seemed to combine better efficiency with simpler operations than other
options.</p>

<p>Pull-based metric gathering puzzled us at first, but we soon realised the
benefits. Initially it seemed like it could be far too heavyweight to scale
well in our environment where we often have several hundred containers with
their own metrics on each host, but by combining it with Telegraf, we can
arrange to have each host export metrics for all its containers (as well as its
overall resource metrics) via a single Prometheus scrape target.</p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>We are a Chef shop so we spun up a largish instance with a big EBS volume and
then reached right for a <a href="https://github.com/rayrod2030/chef-prometheus">community chef
cookbook</a> for Prometheus.</p>

<p>With Prometheus up on a host, we wrote a small Ruby script that uses the Chef
API to query for all our hosts, and write out a Prometheus target config file.
We use this file with a <code>file_sd_config</code> to ensure all hosts are discovered and
scraped as soon as they register with Chef. Thanks to Prometheus’ open
ecosystem, we were able to use Telegraf out of the box with a simple config to
export host-level metrics directly.</p>

<p>We were testing how far a single Prometheus would scale and waiting for it to
fall over. It didn’t! In fact it handled the load of host-level metrics scraped
every 15 seconds for around 450 hosts across our newer infrastructure with very
little resource usage.</p>

<p>We have a lot of containers on each host so we were expecting to have to start
to shard Prometheus once we added all memory usage metrics from those too, but
Prometheus just kept on going without any drama and still without getting too
close to saturating its resources. We currently monitor over 400,000 distinct
metrics every 15 seconds for around 40,000 containers on 450 hosts with a
single m4.xlarge prometheus instance with 1TB of storage. You can see our host
dashboard for this host below. Disk IO on the 1TB gp2 SSD EBS volume will
probably be the limiting factor eventually. Our initial guess is well
over-provisioned for now, but we are growing fast in both metrics gathered and
hosts/containers to monitor.</p>

<p><img src="/assets/blog/2016-09-21/compose-host-dashboard-cb97623cae4.png" alt="Prometheus Host Dashboard"></p>

<p>At this point the Prometheus server we’d thrown up to test with was vastly more
reliable than the InfluxDB cluster we had doing the same job before, so we did
some basic work to make it less of a single-point-of-failure. We added another
identical node scraping all the same targets, then added a simple failover
scheme with keepalived + DNS updates. This was now more highly available than
our previous system so we switched our customer-facing graphs to use Prometheus
and tore down the old system.</p>

<p><img src="/assets/blog/2016-09-21/compose-memory-stats-cb6c2d95184.png" alt="Prometheus-powered memory metrics for PostgresSQL containers in our app"></p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>Our previous monitoring setup was unreliable and difficult to manage. With
Prometheus we have a system that’s working well for graphing lots of metrics,
and we have team members suddenly excited about new ways to use it rather than
wary of touching the metrics system we used before.</p>

<p>The cluster is simpler too, with just two identical nodes. As we grow, we know
we’ll have to shard the work across more Prometheus hosts and have considered a
few ways to do this.</p>

<h2 id="what-do-you-think-the-future-holds-for-compose-and-prometheus?">What do you think the future holds for Compose and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-compose-and-prometheus" name="what-do-you-think-the-future-holds-for-compose-and-prometheus"></a>
</h2>

<p>Right now we have only replicated the metrics we already gathered in previous
systems - basic memory usage for customer containers as well as host-level
resource usage for our own operations. The next logical step is enabling the
database teams to push metrics to the local Telegraf instance from inside the
DB containers so we can record database-level stats too without increasing
number of targets to scrape.</p>

<p>We also have several other systems that we want to get into Prometheus to get
better visibility. We run our apps on Mesos and have integrated basic Docker
container metrics already, which is better than previously, but we also want to
have more of the infrastructure components in the Mesos cluster recording to
the central Prometheus so we can have centralised dashboards showing all
elements of supporting system health from load balancers right down to app
metrics.</p>

<p>Eventually we will need to shard Prometheus. We already split customer
deployments among many smaller clusters for a variety of reasons so the one
logical option would be to move to a smaller Prometheus server (or a pair for
redundancy) per cluster rather than a single global one.</p>

<p>For most reporting needs this is not a big issue as we usually don’t need
hosts/containers from different clusters in the same dashboard, but we may keep
a small global cluster with much longer retention and just a modest number of
down-sampled and aggregated metrics from each cluster’s Prometheus using
Recording Rules.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/09/14/interview-with-digitalocean/">Interview with DigitalOcean</a></h1>
        <aside>Posted at: September 14, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Next in our series of interviews with users of Prometheus, DigitalOcean talks
about how they use Prometheus. Carlos Amedee also talked about <a href="https://www.youtube.com/watch?v=ieo3lGBHcy8">the social
aspects of the rollout</a> at PromCon
2016.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-digitalocean-does?">Can you tell us about yourself and what DigitalOcean does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-digitalocean-does" name="can-you-tell-us-about-yourself-and-what-digitalocean-does"></a>
</h2>

<p>My name is Ian Hansen and I work on the platform metrics team.
<a href="https://www.digitalocean.com/">DigitalOcean</a> provides simple cloud computing.
To date, we’ve created 20 million Droplets (SSD cloud servers) across 13
regions. We also recently released a new Block Storage product.</p>

<p><img src="/assets/blog/2016-09-14/DO_Logo_Horizontal_Blue-3db19536-cb89e8e1298.png" alt="DigitalOcean logo"></p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>Before Prometheus, we were running <a href="https://graphiteapp.org/">Graphite</a> and
<a href="http://opentsdb.net/">OpenTSDB</a>. Graphite was used for smaller-scale
applications and OpenTSDB was used for collecting metrics from all of our
physical servers via <a href="https://collectd.org/">Collectd</a>.
<a href="https://www.nagios.org/">Nagios</a> would pull these databases to trigger alerts.
We do still use Graphite but we no longer run OpenTSDB.</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>I was frustrated with OpenTSDB because I was responsible for keeping the
cluster online, but found it difficult to guard against metric storms.
Sometimes a team would launch a new (very chatty) service that would impact the
total capacity of the cluster and hurt my SLAs. </p>

<p>We are able to blacklist/whitelist new metrics coming in to OpenTSDB, but
didn’t have a great way to guard against chatty services except for
organizational process (which was hard to change/enforce). Other teams were
frustrated with the query language and the visualization tools available at the
time. I was chatting with Julius Volz about push vs pull metric systems and was
sold in wanting to try Prometheus when I saw that I would really be in control
of my SLA when I get to determine what I’m pulling and how frequently. Plus, I
really really liked the query language.</p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>We were gathering metrics via Collectd sending to OpenTSDB. Installing the
<a href="https://github.com/prometheus/node_exporter">Node Exporter</a> in parallel with
our already running Collectd setup allowed us to start experimenting with
Prometheus. We also created a custom exporter to expose Droplet metrics. Soon,
we had feature parity with our OpenTSDB service and started turning off
Collectd and then turned off the OpenTSDB cluster.</p>

<p>People really liked Prometheus and the visualization tools that came with it.
Suddenly, my small metrics team had a backlog that we couldn’t get to fast
enough to make people happy, and instead of providing and maintaining
Prometheus for people’s services, we looked at creating tooling to make it as
easy as possible for other teams to run their own Prometheus servers and to
also run the common exporters we use at the company.</p>

<p>Some teams have started using Alertmanager, but we still have a concept of
pulling Prometheus from our existing monitoring tools.</p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>We’ve improved our insights on hypervisor machines. The data we could get out
of Collectd and Node Exporter is about the same, but it’s much easier for our
team of golang developers to create a new custom exporter that exposes data
specific to the services we run on each hypervisor.</p>

<p>We’re exposing better application metrics. It’s easier to learn and teach how
to create a Prometheus metric that can be aggregated correctly later. With
Graphite it’s easy to create a metric that can’t be aggregated in a certain way
later because the dot-separated-name wasn’t structured right.</p>

<p>Creating alerts is much quicker and simpler than what we had before, plus in a
language that is familiar. This has empowered teams to create better alerting
for the services they know and understand because they can iterate quickly.</p>

<h2 id="what-do-you-think-the-future-holds-for-digitalocean-and-prometheus?">What do you think the future holds for DigitalOcean and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-digitalocean-and-prometheus" name="what-do-you-think-the-future-holds-for-digitalocean-and-prometheus"></a>
</h2>

<p>We’re continuing to look at how to make collecting metrics as easy as possible
for teams at DigitalOcean. Right now teams are running their own Prometheus
servers for the things they care about, which allowed us to gain observability
we otherwise wouldn’t have had as quickly. But, not every team should have to
know how to run Prometheus. We’re looking at what we can do to make Prometheus
as automatic as possible so that teams can just concentrate on what queries and
alerts they want on their services and databases.</p>

<p>We also created <a href="https://github.com/digitalocean/vulcan">Vulcan</a> so that we
have long-term data storage, while retaining the Prometheus Query Language that
we have built tooling around and trained people how to use.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/09/07/interview-with-shuttlecloud/">Interview with ShuttleCloud</a></h1>
        <aside>Posted at: September 7, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, ShuttleCloud talks about how they began using Prometheus. Ignacio from ShuttleCloud also explained how <a href="https://www.youtube.com/watch?v=gMHa4Yh8avk">Prometheus Is Good for Your Small Startup</a> at PromCon 2016.</em></p>

<h2 id="what-does-shuttlecloud-do?">What does ShuttleCloud do?<a class="header-anchor" href="#what-does-shuttlecloud-do" name="what-does-shuttlecloud-do"></a>
</h2>

<p>ShuttleCloud is the world’s most scalable email and contacts data importing system. We help some of the leading email and address book providers, including Google and Comcast, increase user growth and engagement by automating the switching experience through data import. </p>

<p>By integrating our API into their offerings, our customers allow their users to easily migrate their email and contacts from one participating provider to another, reducing the friction users face when switching to a new provider. The 24/7 email providers supported include all major US internet service providers: Comcast, Time Warner Cable, AT&amp;T, Verizon, and more.</p>

<p>By offering end users a simple path for migrating their emails (while keeping complete control over the import tool’s UI), our customers dramatically improve user activation and onboarding.</p>

<p><img src="/assets/blog/2016-09-07/gmail-integration-cbeb0164c27.png" alt="ShuttleCloud's integration with Gmail">
<strong><em>ShuttleCloud’s <a href="https://support.google.com/mail/answer/164640?hl=en">integration</a> with Google’s Gmail Platform.</em></strong> <em>Gmail has imported data for 3 million users with our API.</em></p>

<p>ShuttleCloud’s technology encrypts all the data required to process an import, in addition to following the most secure standards (SSL, oAuth) to ensure the confidentiality and integrity of API requests. Our technology allows us to guarantee our platform’s high availability, with up to 99.5% uptime assurances. </p>

<p><img src="/assets/blog/2016-09-07/shuttlecloud-numbers-cb34514c568.png" alt="ShuttleCloud by Numbers"></p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>In the beginning, a proper monitoring system for our infrastructure was not one of our main priorities. We didn’t have as many projects and instances as we currently have, so we worked with other simple systems to alert us if anything was not working properly and get it under control.</p>

<ul>
<li>We had a set of automatic scripts to monitor most of the operational metrics for the machines. These were cron-based and executed, using Ansible from a centralized machine. The alerts were emails sent directly to the entire development team.</li>
<li>We trusted Pingdom for external blackbox monitoring and checking that all our frontends were up. They provided an easy interface and alerting system in case any of our external services were not reachable.</li>
</ul>

<p>Fortunately, big customers arrived, and the SLAs started to be more demanding. Therefore, we needed something else to measure how we were performing and to ensure that we were complying with all SLAs. One of the features we required was to have accurate stats about our performance and business metrics (i.e., how many migrations finished correctly), so reporting was more on our minds than monitoring. </p>

<p>We developed the following system:</p>

<p><img src="/assets/blog/2016-09-07/Prometheus-System-1-cba8c7f335c.jpg" alt="Initial Shuttlecloud System"></p>

<ul>
<li><p>The source of all necessary data is a status database in a CouchDB. There, each document represents one status of an operation. This information is processed by the Status Importer and stored in a relational manner in a MySQL database.</p></li>
<li>
<p>A component gathers data from that database, with the information aggregated and post-processed into several views. </p>

<ul>
<li>One of the views is the email report, which we needed for reporting purposes. This is sent via email. </li>
<li>The other view pushes data to a dashboard, where it can be easily controlled. The dashboard service we used was external. We trusted Ducksboard, not only because the dashboards were easy to set up and looked beautiful, but also because they provided automatic alerts if a threshold was reached.</li>
</ul>
</li>
</ul>

<p>With all that in place, it didn’t take us long to realize that we would need a proper metrics, monitoring, and alerting system as the number of projects started to increase. </p>

<p>Some drawbacks of the systems we had at that time were:</p>

<ul>
<li>No centralized monitoring system. Each metric type had a different one:

<ul>
<li>System metrics → Scripts run by Ansible.</li>
<li>Business metrics → Ducksboard and email reports.</li>
<li>Blackbox metrics → Pingdom.</li>
</ul>
</li>
<li>No standard alerting system. Each metric type had different alerts (email, push notification, and so on).</li>
<li>Some business metrics had no alerts. These were reviewed manually.</li>
</ul>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>We analyzed several monitoring and alerting systems. We were eager to get our hands dirty and check if the a solution would succeed or fail. The system we decided to put to the test was Prometheus, for the following reasons:</p>

<ul>
<li>First of all, you don’t have to define a fixed metric system to start working with it; metrics can be added or changed in the future. This provides valuable flexibility when you don’t know all of the metrics you want to monitor yet.</li>
<li>If you know anything about Prometheus, you know that metrics can have labels that abstract us from the fact that different time series are considered. This, together with its query language, provided even more flexibility and a powerful tool. For example, we can have the same metric defined for different environments or projects and get a specific time series or aggregate certain metrics with the appropriate labels:

<ul>
<li>
<code>http_requests_total{job="my_super_app_1",environment="staging"}</code> - the time series corresponding to the staging environment for the app "my_super_app_1".</li>
<li>
<code>http_requests_total{job="my_super_app_1"}</code> - the time series for all environments for the app "my_super_app_1".</li>
<li>
<code>http_requests_total{environment="staging"}</code> - the time series for all staging environments for all jobs.</li>
</ul>
</li>
<li>Prometheus supports a DNS service for service discovery. We happened to already  have an internal DNS service.</li>
<li>There is no need to install any external services (unlike Sensu, for example, which needs a data-storage service like Redis and a message bus like RabbitMQ). This might not be a deal breaker, but it definitely makes the test easier to perform, deploy, and maintain.</li>
<li>Prometheus is quite easy to install, as you only need to download an executable Go file. The Docker container also works well and it is easy to start.</li>
</ul>

<h2 id="how-do-you-use-prometheus?">How do you use Prometheus?<a class="header-anchor" href="#how-do-you-use-prometheus" name="how-do-you-use-prometheus"></a>
</h2>

<p>Initially we were only using some metrics provided out of the box by the <a href="https://github.com/prometheus/node_exporter">node_exporter</a>, including:</p>

<ul>
<li>hard drive usage.</li>
<li>memory usage.</li>
<li>if an instance is up or down.</li>
</ul>

<p>Our internal DNS service is integrated to be used for service discovery, so every new instance is automatically monitored.</p>

<p>Some of the metrics we used, which were not provided by the node_exporter by default, were exported using the <a href="https://github.com/prometheus/node_exporter#textfile-collector">node_exporter textfile collector</a> feature. The first alerts we declared on the Prometheus Alertmanager were mainly related to the operational metrics mentioned above.</p>

<p>We later developed an operation exporter that allowed us to know the status of the system almost in real time. It exposed business metrics, namely the statuses of all operations, the number of incoming migrations, the number of finished migrations, and the number of errors. We could aggregate these on the Prometheus side and let it calculate different rates. </p>

<p>We decided to export and monitor the following metrics:</p>

<ul>
<li><code>operation_requests_total</code></li>
<li><code>operation_statuses_total</code></li>
<li><code>operation_errors_total</code></li>
</ul>

<p><img src="/assets/blog/2016-09-07/Prometheus-System-2-cbee2d089c8.jpg" alt="Shuttlecloud Prometheus System"></p>

<p>We have most of our services duplicated in two Google Cloud Platform availability zones. That includes the monitoring system. It’s straightforward to have more than one operation exporter in two or more different zones, as Prometheus can aggregate the data from all of them and make one metric (i.e., the maximum of all). We currently don’t have Prometheus or the Alertmanager in HA — only a metamonitoring instance — but we are working on it.</p>

<p>For external blackbox monitoring, we use the Prometheus <a href="https://github.com/prometheus/blackbox_exporter">Blackbox Exporter</a>. Apart from checking if our external frontends are up, it is especially useful for having metrics for SSL certificates’ expiration dates. It even checks the whole chain of certificates. Kudos to Robust Perception for explaining it perfectly in their <a href="https://www.robustperception.io/get-alerted-before-your-ssl-certificates-expire/">blogpost</a>.</p>

<p>We set up some charts in Grafana for visual monitoring in some dashboards, and the integration with Prometheus was trivial. The query language used to define the charts is the same as in Prometheus, which simplified their creation a lot.</p>

<p>We also integrated Prometheus with Pagerduty and created a schedule of people on-call for the critical alerts. For those alerts that were not considered critical, we only sent an email.</p>

<h2 id="how-does-prometheus-make-things-better-for-you?">How does Prometheus make things better for you?<a class="header-anchor" href="#how-does-prometheus-make-things-better-for-you" name="how-does-prometheus-make-things-better-for-you"></a>
</h2>

<p>We can't compare Prometheus with our previous solution because we didn’t have one, but we can talk about what features of Prometheus are highlights for us:</p>

<ul>
<li>It has very few maintenance requirements.</li>
<li>It’s efficient: one machine can handle monitoring the whole cluster.</li>
<li>The community is friendly—both dev and users. Moreover, <a href="https://www.robustperception.io/blog/">Brian’s blog</a> is a very good resource.</li>
<li>It has no third-party requirements; it’s just the server and the exporters. (No RabbitMQ or Redis needs to be maintained.)</li>
<li>Deployment of Go applications is a breeze.</li>
</ul>

<h2 id="what-do-you-think-the-future-holds-for-shuttlecloud-and-prometheus?">What do you think the future holds for ShuttleCloud and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-shuttlecloud-and-prometheus" name="what-do-you-think-the-future-holds-for-shuttlecloud-and-prometheus"></a>
</h2>

<p>We’re very happy with Prometheus, but new exporters are always welcome (Celery or Spark, for example). </p>

<p>One question that we face every time we add a new alarm is: how do we test that the alarm works as expected? It would be nice to have a way to inject fake metrics in order to raise an alarm, to test it.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/09/04/promcon-2016-its-a-wrap/">PromCon 2016 - It's a wrap!</a></h1>
        <aside>Posted at: September 4, 2016 by Julius Volz</aside>
        <article class="doc-content">
          <h2 id="what-happened">What happened<a class="header-anchor" href="#what-happened" name="what-happened"></a>
</h2>

<p>Last week, eighty Prometheus users and developers from around the world came
together for two days in Berlin for the first-ever conference about the
Prometheus monitoring system: <a href="https://promcon.io/">PromCon 2016</a>. The goal of
this conference was to exchange knowledge, best practices, and experience
gained using Prometheus. We also wanted to grow the community and help people
build professional connections around service monitoring. Here are some
impressions from the first morning:</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/09/04/promcon-2016-its-a-wrap/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/07/23/pull-does-not-scale-or-does-it/">Pull doesn't scale - or does it?</a></h1>
        <aside>Posted at: July 23, 2016 by Julius Volz</aside>
        <article class="doc-content">
          <p>Let's talk about a particularly persistent myth. Whenever there is a discussion
about monitoring systems and Prometheus's pull-based metrics collection
approach comes up, someone inevitably chimes in about how a pull-based approach
just “fundamentally doesn't scale”. The given reasons are often vague or only
apply to systems that are fundamentally different from Prometheus. In fact,
having worked with pull-based monitoring at the largest scales, this claim runs
counter to our own operational experience.</p>

<p>We already have an FAQ entry about
<a href="/docs/introduction/faq/#why-do-you-pull-rather-than-push?">why Prometheus chooses pull over push</a>,
but it does not focus specifically on scaling aspects. Let's have a closer look
at the usual misconceptions around this claim and analyze whether and how they
would apply to Prometheus.</p>

<h2 id="prometheus-is-not-nagios">Prometheus is not Nagios<a class="header-anchor" href="#prometheus-is-not-nagios" name="prometheus-is-not-nagios"></a>
</h2>

<p>When people think of a monitoring system that actively pulls, they often think
of Nagios. Nagios has a reputation of not scaling well, in part due to spawning
subprocesses for active checks that can run arbitrary actions on the Nagios
host in order to determine the health of a certain host or service. This sort
of check architecture indeed does not scale well, as the central Nagios host
quickly gets overwhelmed. As a result, people usually configure checks to only
be executed every couple of minutes, or they run into more serious problems.</p>

<p>However, Prometheus takes a fundamentally different approach altogether.
Instead of executing check scripts, it only collects time series data from a
set of instrumented targets over the network. For each target, the Prometheus
server simply fetches the current state of all metrics of that target over HTTP
(in a highly parallel way, using goroutines) and has no other execution
overhead that would be pull-related. This brings us to the next point:</p>

<h2 id="it-doesn't-matter-who-initiates-the-connection">It doesn't matter who initiates the connection<a class="header-anchor" href="#it-doesn-t-matter-who-initiates-the-connection" name="it-doesn-t-matter-who-initiates-the-connection"></a>
</h2>

<p>For scaling purposes, it doesn't matter who initiates the TCP connection over
which metrics are then transferred. Either way you do it, the effort for
establishing a connection is small compared to the metrics payload and other
required work.</p>

<p>But a push-based approach could use UDP and avoid connection establishment
altogether, you say! True, but the TCP/HTTP overhead in Prometheus is still
negligible compared to the other work that the Prometheus server has to do to
ingest data (especially persisting time series data on disk). To put some
numbers behind this: a single big Prometheus server can easily store millions
of time series, with a record of 800,000 incoming samples per second (as
measured with real production metrics data at SoundCloud). Given a 10-seconds
scrape interval and 700 time series per host, this allows you to monitor over
10,000 machines from a single Prometheus server. The scaling bottleneck here
has never been related to pulling metrics, but usually to the speed at which
the Prometheus server can ingest the data into memory and then sustainably
persist and expire data on disk/SSD.</p>

<p>Also, although networks are pretty reliable these days, using a TCP-based pull
approach makes sure that metrics data arrives reliably, or that the monitoring
system at least knows immediately when the metrics transfer fails due to a
broken network.</p>

<h2 id="prometheus-is-not-an-event-based-system">Prometheus is not an event-based system<a class="header-anchor" href="#prometheus-is-not-an-event-based-system" name="prometheus-is-not-an-event-based-system"></a>
</h2>

<p>Some monitoring systems are event-based. That is, they report each individual
event (an HTTP request, an exception, you name it) to a central monitoring
system immediately as it happens. This central system then either aggregates
the events into metrics (StatsD is the prime example of this) or stores events
individually for later processing (the ELK stack is an example of that). In
such a system, pulling would be problematic indeed: the instrumented service
would have to buffer events between pulls, and the pulls would have to happen
incredibly frequently in order to simulate the same “liveness” of the
push-based approach and not overwhelm event buffers.</p>

<p>However, again, Prometheus is not an event-based monitoring system. You do not
send raw events to Prometheus, nor can it store them. Prometheus is in the
business of collecting aggregated time series data. That means that it's only
interested in regularly collecting the current <em>state</em> of a given set of
metrics, not the underlying events that led to the generation of those metrics.
For example, an instrumented service would not send a message about each HTTP
request to Prometheus as it is handled, but would simply count up those
requests in memory.  This can happen hundreds of thousands of times per second
without causing any monitoring traffic. Prometheus then simply asks the service
instance every 15 or 30 seconds (or whatever you configure) about the current
counter value and stores that value together with the scrape timestamp as a
sample. Other metric types, such as gauges, histograms, and summaries, are
handled similarly. The resulting monitoring traffic is low, and the pull-based
approach also does not create problems in this case.</p>

<h2 id="but-now-my-monitoring-needs-to-know-about-my-service-instances!">But now my monitoring needs to know about my service instances!<a class="header-anchor" href="#but-now-my-monitoring-needs-to-know-about-my-service-instances" name="but-now-my-monitoring-needs-to-know-about-my-service-instances"></a>
</h2>

<p>With a pull-based approach, your monitoring system needs to know which service
instances exist and how to connect to them. Some people are worried about the
extra configuration this requires on the part of the monitoring system and see
this as an operational scalability problem.</p>

<p>We would argue that you cannot escape this configuration effort for
serious monitoring setups in any case: if your monitoring system doesn't know
what the world <em>should</em> look like and which monitored service instances
<em>should</em> be there, how would it be able to tell when an instance just never
reports in, is down due to an outage, or really is no longer meant to exist?
This is only acceptable if you never care about the health of individual
instances at all, like when you only run ephemeral workers where it is
sufficient for a large-enough number of them to report in some result. Most
environments are not exclusively like that.</p>

<p>If the monitoring system needs to know the desired state of the world anyway,
then a push-based approach actually requires <em>more</em> configuration in total. Not
only does your monitoring system need to know what service instances should
exist, but your service instances now also need to know how to reach your
monitoring system. A pull approach not only requires less configuration,
it also makes your monitoring setup more flexible. With pull, you can just run
a copy of production monitoring on your laptop to experiment with it. It also
allows you just fetch metrics with some other tool or inspect metrics endpoints
manually. To get high availability, pull allows you to just run two identically
configured Prometheus servers in parallel. And lastly, if you have to move the
endpoint under which your monitoring is reachable, a pull approach does not
require you to reconfigure all of your metrics sources.</p>

<p>On a practical front, Prometheus makes it easy to configure the desired state
of the world with its built-in support for a wide variety of service discovery
mechanisms for cloud providers and container-scheduling systems: Consul,
Marathon, Kubernetes, EC2, DNS-based SD, Azure, Zookeeper Serversets, and more.
Prometheus also allows you to plug in your own custom mechanism if needed.
In a microservice world or any multi-tiered architecture, it is also
fundamentally an advantage if your monitoring system uses the same method to
discover targets to monitor as your service instances use to discover their
backends. This way you can be sure that you are monitoring the same targets
that are serving production traffic and you have only one discovery mechanism
to maintain.</p>

<h2 id="accidentally-ddos-ing-your-monitoring">Accidentally DDoS-ing your monitoring<a class="header-anchor" href="#accidentally-ddos-ing-your-monitoring" name="accidentally-ddos-ing-your-monitoring"></a>
</h2>

<p>Whether you pull or push, any time-series database will fall over if you send
it more samples than it can handle. However, in our experience it's slightly
more likely for a push-based approach to accidentally bring down your
monitoring. If the control over what metrics get ingested from which instances
is not centralized (in your monitoring system), then you run into the danger of
experimental or rogue jobs suddenly pushing lots of garbage data into your
production monitoring and bringing it down.  There are still plenty of ways how
this can happen with a pull-based approach (which only controls where to pull
metrics from, but not the size and nature of the metrics payloads), but the
risk is lower. More importantly, such incidents can be mitigated at a central
point.</p>

<h2 id="real-world-proof">Real-world proof<a class="header-anchor" href="#real-world-proof" name="real-world-proof"></a>
</h2>

<p>Besides the fact that Prometheus is already being used to monitor very large
setups in the real world (like using it to <a href="http://promcon.io/talks/scaling_to_a_million_machines_with_prometheus/">monitor millions of machines at
DigitalOcean</a>),
there are other prominent examples of pull-based monitoring being used
successfully in the largest possible environments. Prometheus was inspired by
Google's Borgmon, which was (and partially still is) used within Google to
monitor all its critical production services using a pull-based approach. Any
scaling issues we encountered with Borgmon at Google were not due its pull
approach either. If a pull-based approach scales to a global environment with
many tens of datacenters and millions of machines, you can hardly say that pull
doesn't scale.</p>

<h2 id="but-there-are-other-problems-with-pull!">But there are other problems with pull!<a class="header-anchor" href="#but-there-are-other-problems-with-pull" name="but-there-are-other-problems-with-pull"></a>
</h2>

<p>There are indeed setups that are hard to monitor with a pull-based approach.
A prominent example is when you have many endpoints scattered around the
world which are not directly reachable due to firewalls or complicated
networking setups, and where it's infeasible to run a Prometheus server
directly in each of the network segments. This is not quite the environment for
which Prometheus was built, although workarounds are often possible (<a href="/docs/practices/pushing/">via the
Pushgateway or restructuring your setup</a>). In any
case, these remaining concerns about pull-based monitoring are usually not
scaling-related, but due to network operation difficulties around opening TCP
connections.</p>

<h2 id="all-good-then?">All good then?<a class="header-anchor" href="#all-good-then" name="all-good-then"></a>
</h2>

<p>This article addresses the most common scalability concerns around a pull-based
monitoring approach. With Prometheus and other pull-based systems being used
successfully in very large environments and the pull aspect not posing a
bottleneck in reality, the result should be clear: the “pull doesn't scale”
argument is not a real concern. We hope that future debates will focus on
aspects that matter more than this red herring.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/07/18/prometheus-1-0-released/">Prometheus reaches 1.0</a></h1>
        <aside>Posted at: July 18, 2016 by Fabian Reinartz on behalf of the Prometheus team</aside>
        <article class="doc-content">
          <p>In January, we published a blog post on <a href="https://prometheus.io/blog/2016/01/26/one-year-of-open-prometheus-development/">Prometheus’s first year of public existence</a>, summarizing what has been an amazing journey for us, and hopefully an innovative and useful monitoring solution for you.
Since then, <a href="https://prometheus.io/blog/2016/05/09/prometheus-to-join-the-cloud-native-computing-foundation/">Prometheus has also joined the Cloud Native Computing Foundation</a>, where we are in good company, as the second charter project after <a href="http://kubernetes.io/">Kubernetes</a>.</p>

<p>Our recent work has focused on delivering a stable API and user interface, marked by version 1.0 of Prometheus.
We’re thrilled to announce that we’ve reached this goal, and <a href="https://github.com/prometheus/prometheus/releases/tag/v1.0.0">Prometheus 1.0 is available today</a>.</p>

<h2 id="what-does-1.0-mean-for-you?">What does 1.0 mean for you?<a class="header-anchor" href="#what-does-1-0-mean-for-you" name="what-does-1-0-mean-for-you"></a>
</h2>

<p>If you have been using Prometheus for a while, you may have noticed that the rate and impact of breaking changes significantly decreased over the past year.
In the same spirit, reaching 1.0 means that subsequent 1.x releases will remain API stable. Upgrades won’t break programs built atop the Prometheus API, and updates won’t require storage re-initialization or deployment changes. Custom dashboards and alerts will remain intact across 1.x version updates as well.
We’re confident Prometheus 1.0 is a solid monitoring solution. Now that the Prometheus server has reached a stable API state, other modules will follow it to their own stable version 1.0 releases over time.</p>

<h3 id="fine-print">Fine print<a class="header-anchor" href="#fine-print" name="fine-print"></a>
</h3>

<p>So what does API stability mean? Prometheus has a large surface area and some parts are certainly more mature than others.
There are two simple categories, <em>stable</em> and <em>unstable</em>:</p>

<p>Stable as of v1.0 and throughout the 1.x series:</p>

<ul>
<li>The query language and data model</li>
<li>Alerting and recording rules</li>
<li>The ingestion exposition formats</li>
<li>Configuration flag names</li>
<li>HTTP API (used by dashboards and UIs)</li>
<li>Configuration file format (minus the non-stable service discovery integrations, see below)</li>
<li>Alerting integration with Alertmanager 0.1+ for the foreseeable future</li>
<li>Console template syntax and semantics</li>
</ul>

<p>Unstable and may change within 1.x:</p>

<ul>
<li>The remote storage integrations (InfluxDB, OpenTSDB, Graphite) are still experimental and will at some point be removed in favor of a generic, more sophisticated API that allows storing samples in arbitrary storage systems.</li>
<li>Several service discovery integrations are new and need to keep up with fast evolving systems. Hence, integrations with Kubernetes, Marathon, Azure, and EC2 remain in beta status and are subject to change. However, changes will be clearly announced.</li>
<li>Exact flag meanings may change as necessary. However, changes will never cause the server to not start with previous flag configurations.</li>
<li>Go APIs of packages that are part of the server.</li>
<li>HTML generated by the web UI.</li>
<li>The metrics in the <code>/metrics</code> endpoint of Prometheus itself.</li>
<li>Exact on-disk format. Potential changes however, will be forward compatible and transparently handled by Prometheus.</li>
</ul>

<h2 id="so-prometheus-is-complete-now?">So Prometheus is complete now?<a class="header-anchor" href="#so-prometheus-is-complete-now" name="so-prometheus-is-complete-now"></a>
</h2>

<p>Absolutely not. We have a long roadmap ahead of us, full of great features to implement. Prometheus will not stay in 1.x for years to come. The infrastructure space is evolving rapidly and we fully intend for Prometheus to evolve with it.
This means that we will remain willing to question what we did in the past and are open to leave behind things that have lost relevance. There will be new major versions of Prometheus to facilitate future plans like persistent long-term storage, newer iterations of Alertmanager, internal storage improvements, and many things we don’t even know about yet.</p>

<h2 id="closing-thoughts">Closing thoughts<a class="header-anchor" href="#closing-thoughts" name="closing-thoughts"></a>
</h2>

<p>We want to thank our fantastic community for field testing new versions, filing bug reports, contributing code, helping out other community members, and shaping Prometheus by participating in countless productive discussions.
In the end, you are the ones who make Prometheus successful.</p>

<p>Thank you, and keep up the great work!</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/05/09/prometheus-to-join-the-cloud-native-computing-foundation/">Prometheus to Join the Cloud Native Computing Foundation</a></h1>
        <aside>Posted at: May 9, 2016 by Julius Volz on behalf of the Prometheus core developers</aside>
        <article class="doc-content">
          <p>Since the inception of Prometheus, we have been looking for a sustainable
governance model for the project that is independent of any single company.
Recently, we have been in discussions with the newly formed <a href="https://cncf.io/">Cloud Native
Computing Foundation</a> (CNCF), which is backed by Google,
CoreOS, Docker, Weaveworks, Mesosphere, and <a href="https://cncf.io/about/members">other leading infrastructure
companies</a>.</p>

<p>Today, we are excited to announce that the CNCF's Technical Oversight Committee
<a href="http://lists.cncf.io/pipermail/cncf-toc/2016-May/000198.html">voted unanimously</a> to
accept Prometheus as a second hosted project after Kubernetes! You can find
more information about these plans in the
<a href="https://cncf.io/news/news/2016/05/cloud-native-computing-foundation-accepts-prometheus-second-hosted-project">official press release by the CNCF</a>.</p>

<p>By joining the CNCF, we hope to establish a clear and sustainable project
governance model, as well as benefit from the resources, infrastructure, and
advice that the independent foundation provides to its members.</p>

<p>We think that the CNCF and Prometheus are an ideal thematic match, as both
focus on bringing about a modern vision of the cloud.</p>

<p>In the following months, we will be working with the CNCF on finalizing the
project governance structure. We will report back when there are more details
to announce.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/05/08/when-to-use-varbit-chunks/">When (not) to use varbit chunks</a></h1>
        <aside>Posted at: May 8, 2016 by Björn “Beorn” Rabenstein</aside>
        <article class="doc-content">
          <p>The embedded time serie database (TSDB) of the Prometheus server organizes the
raw sample data of each time series in chunks of constant 1024 bytes size. In
addition to the raw sample data, a chunk contains some meta-data, which allows
the selection of a different encoding for each chunk. The most fundamental
distinction is the encoding version. You select the version for newly created
chunks via the command line flag <code>-storage.local.chunk-encoding-version</code>. Up to
now, there were only two supported versions: 0 for the original delta encoding,
and 1 for the improved double-delta encoding. With release
<a href="https://github.com/prometheus/prometheus/releases/tag/0.18.0">0.18.0</a>, we
added version 2, which is another variety of double-delta encoding. We call it
<em>varbit encoding</em> because it involves a variable bit-width per sample within
the chunk. While version 1 is superior to version 0 in almost every aspect,
there is a real trade-off between version 1 and 2. This blog post will help you
to make that decision. Version 1 remains the default encoding, so if you want
to try out version 2 after reading this article, you have to select it
explicitly via the command line flag. There is no harm in switching back and
forth, but note that existing chunks will not change their encoding version
once they have been created. However, these chunks will gradually be phased out
according to the configured retention time and will thus be replaced by chunks
with the encoding specified in the command-line flag.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/05/08/when-to-use-varbit-chunks/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/05/01/interview-with-showmax/">Interview with ShowMax</a></h1>
        <aside>Posted at: May 1, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>This is the second in a series of interviews with users of Prometheus, allowing
them to share their experiences of evaluating and using Prometheus.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-showmax-does?">Can you tell us about yourself and what ShowMax does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-showmax-does" name="can-you-tell-us-about-yourself-and-what-showmax-does"></a>
</h2>

<p>I’m Antonin Kral, and I’m leading research and architecture for
<a href="http://www.showmax.com">ShowMax</a>. Before that, I’ve held architectural and CTO
roles for the past 12 years.</p>

<p>ShowMax is a subscription video on demand service that launched in South Africa
in 2015. We’ve got an extensive content catalogue with more than 20,000
episodes of TV shows and movies. Our service is currently available in 65
countries worldwide. While better known rivals are skirmishing in America and
Europe, ShowMax is battling a more difficult problem: how do you binge-watch
in a barely connected village in sub-Saharan Africa? Already 35% of video
around the world is streamed, but there are still so many places the revolution
has left untouched.</p>

<p><img src="/assets/blog/2016-05-01/showmax-logo-cb5f41c49fe.png" alt="ShowMax logo"></p>

<p>We are managing about 50 services running mostly on private clusters built
around CoreOS. They are primarily handling API requests from our clients
(Android, iOS, AppleTV, JavaScript, Samsung TV, LG TV etc), while some of them
are used internally. One of the biggest internal pipelines is video encoding
which can occupy 400+ physical servers when handling large ingestion batches.</p>

<p>The majority of our back-end services are written in Ruby, Go or Python. We use
EventMachine when writing apps in Ruby (Goliath on MRI, Puma on JRuby). Go is
typically used in apps that require large throughput and don’t have so much
business logic. We’re very happy with Falcon for services written in Python.
Data is stored in PostgreSQL and ElasticSearch clusters. We use etcd and custom
tooling for configuring Varnishes for routing requests.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/05/01/interview-with-showmax/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/03/23/interview-with-life360/">Interview with Life360</a></h1>
        <aside>Posted at: March 23, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>This is the first in a series of interviews with users of Prometheus, allowing
them to share their experiences of evaluating and using Prometheus. Our first
interview is with Daniel from Life360.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-life360-does?">Can you tell us about yourself and what Life360 does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-life360-does" name="can-you-tell-us-about-yourself-and-what-life360-does"></a>
</h2>

<p>I’m Daniel Ben Yosef, a.k.a, dby, and I’m an Infrastructure Engineer for
<a href="https://www.life360.com/">Life360</a>, and before that, I’ve held systems
engineering roles for the past 9 years.</p>

<p>Life360 creates technology that helps families stay connected, we’re the Family
Network app for families. We’re quite busy handling these families - at peak
we serve 700k requests per minute for 70 million registered families.</p>

<p><a href="https://www.life360.com/"><img src="/assets/blog/2016-03-23/life360_horizontal_logo_gradient_rgb-cbd4306cf57.png" style="width: 444px; height:177px"></a></p>

<p>We manage around 20 services in production, mostly handling location requests
from mobile clients (Android, iOS, and Windows Phone), spanning over 150+
instances at peak. Redundancy and high-availability are our goals and we strive
to maintain 100% uptime whenever possible because families trust us to be
available.</p>

<p>We hold user data in both our MySQL multi-master cluster and in our 12-node
Cassandra ring which holds around 4TB of data at any given time. We have
services written in Go, Python, PHP, as well as plans to introduce Java to our
stack. We use Consul for service discovery, and of course our Prometheus setup
is integrated with it.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/03/23/interview-with-life360/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/03/03/custom-alertmanager-templates/">Custom Alertmanager Templates</a></h1>
        <aside>Posted at: March 3, 2016 by Fabian Reinartz</aside>
        <article class="doc-content">
          <p>The Alertmanager handles alerts sent by Prometheus servers and sends
notifications about them to different receivers based on their labels.</p>

<p>A receiver can be one of many different integrations such as PagerDuty, Slack,
email, or a custom integration via the generic webhook interface (for example <a href="https://github.com/fabxc/jiralerts">JIRA</a>).</p>

<h2 id="templates">Templates<a class="header-anchor" href="#templates" name="templates"></a>
</h2>

<p>The messages sent to receivers are constructed via templates.
Alertmanager comes with default templates but also allows defining custom
ones.</p>

<p>In this blog post, we will walk through a simple customization of Slack
notifications.</p>

<p>We use this simple Alertmanager configuration that sends all alerts to Slack:</p>

<pre><code class="yaml">global:
  slack_api_url: '&lt;slack_webhook_url&gt;'

route:
  receiver: 'slack-notifications'
  # All alerts in a notification have the same value for these labels.
  group_by: [alertname, datacenter, app]

receivers:
- name: 'slack-notifications'
  slack_configs:
  - channel: '#alerts'
</code></pre>

<p>By default, a Slack message sent by Alertmanager looks like this:</p>

<p><img src="/assets/blog/2016-03-03/slack_alert_before-cb51e526b7f.png" alt=""></p>

<p>It shows us that there is one firing alert, followed by the label values of
the alert grouping (alertname, datacenter, app) and further label values the
alerts have in common (critical).</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/03/03/custom-alertmanager-templates/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/01/26/one-year-of-open-prometheus-development/">One Year of Open Prometheus Development</a></h1>
        <aside>Posted at: January 26, 2016 by Julius Volz</aside>
        <article class="doc-content">
          <h2 id="the-beginning">The beginning<a class="header-anchor" href="#the-beginning" name="the-beginning"></a>
</h2>

<p>A year ago today, we officially announced Prometheus to the wider world. This
is a great opportunity for us to look back and share some of the wonderful
things that have happened to the project since then. But first, let's start at
the beginning.</p>

<p>Although we had already started Prometheus as an open-source project on GitHub in
2012, we didn't make noise about it at first. We wanted to give the project
time to mature and be able to experiment without friction. Prometheus was
gradually introduced for production monitoring at
<a href="https://soundcloud.com/">SoundCloud</a> in 2013 and then saw more and more
usage within the company, as well as some early adoption by our friends at
Docker and Boexever in 2014. Over the years, Prometheus was growing more and
more mature and although it was already solving people's monitoring problems,
it was still unknown to the wider public.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/01/26/one-year-of-open-prometheus-development/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2015/08/17/service-discovery-with-etcd/">Custom service discovery with etcd</a></h1>
        <aside>Posted at: August 17, 2015 by Fabian Reinartz</aside>
        <article class="doc-content">
          <p>In a <a href="/blog/2015/06/01/advanced-service-discovery/">previous post</a> we
introduced numerous new ways of doing service discovery in Prometheus.
Since then a lot has happened. We improved the internal implementation and
received fantastic contributions from our community, adding support for
service discovery with Kubernetes and Marathon. They will become available
with the release of version 0.16.</p>

<p>We also touched on the topic of <a href="/blog/2015/06/01/advanced-service-discovery/#custom-service-discovery">custom service discovery</a>.</p>

<p>Not every type of service discovery is generic enough to be directly included
in Prometheus. Chances are your organisation has a proprietary
system in place and you just have to make it work with Prometheus.
This does not mean that you cannot enjoy the benefits of automatically
discovering new monitoring targets.</p>

<p>In this post we will implement a small utility program that connects a custom
service discovery approach based on <a href="https://coreos.com/etcd/">etcd</a>, the
highly consistent distributed key-value store, to Prometheus.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2015/08/17/service-discovery-with-etcd/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2015/06/24/monitoring-dreamhack/">Monitoring DreamHack - the World's Largest Digital Festival</a></h1>
        <aside>Posted at: June 24, 2015 by Christian Svensson (DreamHack Network Team)</aside>
        <article class="doc-content">
          <p><em>Editor's note: This article is a guest post written by a Prometheus user.</em></p>

<p><strong>If you are operating the network for 10,000's of demanding gamers, you need to
really know what is going on inside your network. Oh, and everything needs to be
built from scratch in just five days.</strong></p>

<p>If you have never heard about <a href="http://www.dreamhack.se/">DreamHack</a> before, here
is the pitch: Bring 20,000 people together and have the majority of them bring
their own computer.  Mix in professional gaming (eSports), programming contests,
and live music concerts. The result is the world's largest festival dedicated
solely to everything digital.</p>

<p>To make such an event possible, there needs to be a lot of infrastructure in
place. Ordinary infrastructures of this size take months to build, but the crew
at DreamHack builds everything from scratch in just five days. This of course
includes stuff like configuring network switches, but also building the
electricity distribution, setting up stores for food and drinks, and even
building the actual tables.</p>

<p>The team that builds and operates everything related to the network is
officially called the Network team, but we usually refer to ourselves as <em>tech</em>
or <em>dhtech</em>. This post is going to focus on the work of dhtech and how we used
Prometheus during DreamHack Summer 2015 to try to kick our monitoring up another
notch.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2015/06/24/monitoring-dreamhack/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2015/06/18/practical-anomaly-detection/">Practical Anomaly Detection</a></h1>
        <aside>Posted at: June 18, 2015 by Brian Brazil</aside>
        <article class="doc-content">
          <p>In his <em><a href="http://www.kitchensoap.com/2015/05/01/openlettertomonitoringproducts/">Open Letter To Monitoring/Metrics/Alerting Companies</a></em>,
John Allspaw asserts that attempting "to detect anomalies perfectly, at the right time, is not possible".</p>

<p>I have seen several attempts by talented engineers to build systems to
automatically detect and diagnose problems based on time series data. While it
is certainly possible to get a demonstration working, the data always turned
out to be too noisy to make this approach work for anything but the simplest of
real-world systems.</p>

<p>All hope is not lost though. There are many common anomalies which you can
detect and handle with custom-built rules. The Prometheus <a href="../../../../../docs/querying/basics/">query
language</a> gives you the tools to discover
these anomalies while avoiding false positives.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2015/06/18/practical-anomaly-detection/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2015/06/01/advanced-service-discovery/">Advanced Service Discovery in Prometheus 0.14.0</a></h1>
        <aside>Posted at: June 1, 2015 by Fabian Reinartz, Julius Volz</aside>
        <article class="doc-content">
          <p>This week we released Prometheus v0.14.0 — a version with many long-awaited additions
and improvements.</p>

<p>On the user side, Prometheus now supports new service discovery mechanisms. In
addition to DNS-SRV records, it now supports <a href="https://www.consul.io">Consul</a>
out of the box, and a file-based interface allows you to connect your own
discovery mechanisms. Over time, we plan to add other common service discovery
mechanisms to Prometheus.</p>

<p>Aside from many smaller fixes and improvements, you can now also reload your configuration during
runtime by sending a <code>SIGHUP</code> to the Prometheus process. For a full list of changes, check the
<a href="https://github.com/prometheus/prometheus/blob/master/CHANGELOG.md#0140--2015-06-01">changelog for this release</a>.</p>

<p>In this blog post, we will take a closer look at the built-in service discovery mechanisms and provide
some practical examples. As an additional resource, see
<a href="/docs/operating/configuration">Prometheus's configuration documentation</a>.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2015/06/01/advanced-service-discovery/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2015/04/24/prometheus-monitring-spreads-through-the-internet/">Prometheus Monitoring Spreads through the Internet</a></h1>
        <aside>Posted at: April 24, 2015 by Brian Brazil</aside>
        <article class="doc-content">
          <p>It has been almost three months since we publicly announced Prometheus version
0.10.0, and we're now at version 0.13.1.</p>

<p><a href="https://developers.soundcloud.com/blog/prometheus-monitoring-at-soundcloud">SoundCloud's announcement blog post</a>
remains the best overview of the key components of Prometheus, but there has
been a lot of other online activity around Prometheus. This post will let you
catch up on anything you missed.</p>

<p>In the future, we will use this blog to publish more articles and announcements
to help you get the most out of Prometheus.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2015/04/24/prometheus-monitring-spreads-through-the-internet/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
  </div>
  <div class="col-md-3 side-nav-col">
  <ul class="nav navbar-nav side-nav">
    <li>
      <span class="nav-header">Blog posts</span>
      <ul class="nav active">
      
        <li><a href="/blog/2017/09/04/promcon-2017-recap/">PromCon 2017 Recap</a></li>
      
        <li><a href="/blog/2017/06/21/prometheus-20-alpha3-new-rule-format/">Prometheus 2.0 Alpha.3 with New Rule Format</a></li>
      
        <li><a href="/blog/2017/06/14/interview-with-latelier-animation/">Interview with L’Atelier Animation</a></li>
      
        <li><a href="/blog/2017/05/17/interview-with-iadvize/">Interview with iAdvize</a></li>
      
        <li><a href="/blog/2017/04/10/promehteus-20-sneak-peak/">Sneak Peak of Prometheus 2.0</a></li>
      
        <li><a href="/blog/2017/04/06/interview-with-europace/">Interview with Europace</a></li>
      
        <li><a href="/blog/2017/02/20/interview-with-weaveworks/">Interview with Weaveworks</a></li>
      
        <li><a href="/blog/2016/11/16/interview-with-canonical/">Interview with Canonical</a></li>
      
        <li><a href="/blog/2016/10/12/interview-with-justwatch/">Interview with JustWatch</a></li>
      
        <li><a href="/blog/2016/09/21/interview-with-compose/">Interview with Compose</a></li>
      
        <li><a href="/blog/2016/09/14/interview-with-digitalocean/">Interview with DigitalOcean</a></li>
      
        <li><a href="/blog/2016/09/07/interview-with-shuttlecloud/">Interview with ShuttleCloud</a></li>
      
        <li><a href="/blog/2016/09/04/promcon-2016-its-a-wrap/">PromCon 2016 - It's a wrap!</a></li>
      
        <li><a href="/blog/2016/07/23/pull-does-not-scale-or-does-it/">Pull doesn't scale - or does it?</a></li>
      
        <li><a href="/blog/2016/07/18/prometheus-1-0-released/">Prometheus reaches 1.0</a></li>
      
        <li><a href="/blog/2016/05/09/prometheus-to-join-the-cloud-native-computing-foundation/">Prometheus to Join the Cloud Native Computing Foundation</a></li>
      
        <li><a href="/blog/2016/05/08/when-to-use-varbit-chunks/">When (not) to use varbit chunks</a></li>
      
        <li><a href="/blog/2016/05/01/interview-with-showmax/">Interview with ShowMax</a></li>
      
        <li><a href="/blog/2016/03/23/interview-with-life360/">Interview with Life360</a></li>
      
        <li><a href="/blog/2016/03/03/custom-alertmanager-templates/">Custom Alertmanager Templates</a></li>
      
        <li><a href="/blog/2016/01/26/one-year-of-open-prometheus-development/">One Year of Open Prometheus Development</a></li>
      
        <li><a href="/blog/2015/08/17/service-discovery-with-etcd/">Custom service discovery with etcd</a></li>
      
        <li><a href="/blog/2015/06/24/monitoring-dreamhack/">Monitoring DreamHack - the World's Largest Digital Festival</a></li>
      
        <li><a href="/blog/2015/06/18/practical-anomaly-detection/">Practical Anomaly Detection</a></li>
      
        <li><a href="/blog/2015/06/01/advanced-service-discovery/">Advanced Service Discovery in Prometheus 0.14.0</a></li>
      
        <li><a href="/blog/2015/04/24/prometheus-monitring-spreads-through-the-internet/">Prometheus Monitoring Spreads through the Internet</a></li>
      
      </ul>
    </li>
  </ul>
</div>

</div>

  <hr>

<footer>
  <p class="pull-left">
    &copy; Prometheus Authors 2017
  </p>
</footer>

</div>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-2.2.2.min.js" integrity="sha256-36cp2Co+/62rEAAYHLmRCPIych47CvdM+uTBJwSzWjI=" crossorigin="anonymous"></script>
    <script src="/assets/bootstrap-3.3.1/js/bootstrap.min-cb2616d3564.js"></script>
    <script src="/assets/docs-cb53fb1bfd3.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="/assets/ie10-viewport-bug-workaround-cbb5a0dd7ce.js"></script>
    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-58468480-1', 'auto');
      ga('send', 'pageview');
    </script>
  </body>
</html>

