<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="An open-source monitoring system with a dimensional data model, flexible query language, efficient time series database and modern alerting approach.">
    <meta name="keywords" content="prometheus, monitoring, monitoring system, time series, time series database, alerting, metrics, telemetry">
    <meta name="author" content="Prometheus">

    <link rel="alternate" type="application/atom+xml" title="Prometheus Blog » Feed" href="/blog/feed.xml">

    <link rel="shortcut icon" href="/assets/favicons/favicon.ico">
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/favicons/apple-touch-icon-57x57-cbe16921e26.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/favicons/apple-touch-icon-60x60-cb34684d423.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/favicons/apple-touch-icon-72x72-cb804bd5fae.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/favicons/apple-touch-icon-76x76-cb4a7d77a78.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/favicons/apple-touch-icon-114x114-cbb4144a5d4.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/favicons/apple-touch-icon-120x120-cb24b29faf0.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/favicons/apple-touch-icon-144x144-cb590f03c01.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/favicons/apple-touch-icon-152x152-cbb45d875fa.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicons/apple-touch-icon-180x180-cb5da064c37.png">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-32x32-cb6bc898e38.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/assets/favicons/android-chrome-192x192-cbd9b99e56f.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-96x96-cbe466ffcf9.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/assets/favicons/favicon-16x16-cb02406aa58.png" sizes="16x16">
    <link rel="manifest" href="/assets/favicons/android-chrome-manifest.json">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-TileImage" content="/assets/favicons/mstile-144x144.png">
    <meta name="theme-color" content="#ffffff">

    
    <title>Blog | Prometheus</title>
    

    <!-- Bootstrap core CSS -->
    <link href="/assets/bootstrap-3.3.1/css/bootstrap.min-cb3ab3438f8.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="/css/docs-cbd81c8f773.css" rel="stylesheet">
    <link href="/css/routing-tree-editor-cbd4d13cac6.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="/assets/font-awesome-4.2.0/css/font-awesome.min-cbfeda974a7.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Lato:300,300italic,400' rel='stylesheet' type='text/css'>

  </head>

  <body>

  <div class="">
    <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/"><img src="/assets/prometheus_logo_grey.svg" alt="Prometheus logo"> Prometheus</a>
        </div>
        <div class="collapse navbar-collapse" id="navbar">
          <ul class="nav navbar-nav navbar-right main-nav">
            <li><a href="/docs/introduction/overview/">Docs</a></li>
            <li><a href="/download/">Download</a></li>
            <li><a href="/community/">Community</a></li>
            <li><a href="/blog/">Blog</a></li>
            <li><a href="https://github.com/prometheus"><i class="fa fa-github"></i></a></li>
            <li><a href="https://twitter.com/PrometheusIO"><i class="fa fa-twitter"></i></a></li>
          </ul>
        </div>
      </div>
    </nav>
  </div>


<div class="container">
  <div class="row">
  <div class="col-md-9">
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/11/16/interview-with-canonical/">Interview with Canonical</a></h1>
        <aside>Posted at: November 16, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, Canonical talks
about how they are transitioning to Prometheus.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-canonical-does?">Can you tell us about yourself and what Canonical does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-canonical-does" name="can-you-tell-us-about-yourself-and-what-canonical-does"></a>
</h2>

<p><a href="http://www.canonical.com/">Canonical</a> is probably best known as the company
that sponsors Ubuntu Linux.  We also produce or contribute to a number of other
open-source projects including MAAS, Juju, and OpenStack, and provide
commercial support for these products.  Ubuntu powers the majority of OpenStack
deployments, with 55% of production clouds and <a href="https://www.openstack.org/assets/survey/April-2016-User-Survey-Report.pdf#page=47">58% of large cloud
deployments</a>.</p>

<p>My group, BootStack, is our fully managed private cloud service.  We build and
operate OpenStack clouds for Canonical customers.</p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>We’d used a combination of <a href="https://www.nagios.org/">Nagios</a>,
<a href="https://graphite.readthedocs.io/en/latest/">Graphite</a>/<a href="https://github.com/etsy/statsd">statsd</a>,
and in-house <a href="https://www.djangoproject.com/">Django</a> apps. These did not offer
us the level of flexibility and reporting that we need in both our internal and
customer cloud environments.</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>We’d evaluated a few alternatives, including
<a href="https://github.com/influxdata/influxdb">InfluxDB</a> and extending our use of
Graphite, but our first experiences with Prometheus proved it to have the
combination of simplicity and power that we were looking for.  We especially
appreciate the convenience of labels, the simple HTTP protocol, and the out of
box <a href="https://prometheus.io/docs/alerting/rules/">timeseries alerting</a>. The
potential with Prometheus to replace 2 different tools (alerting and trending)
with one is particularly appealing.</p>

<p>Also, several of our staff have prior experience with Borgmon from their time
at Google which greatly added to our interest!</p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>We are still in the process of transitioning, we expect this will take some
time due to the number of custom checks we currently use in our existing
systems that will need to be re-implemented in Prometheus.  The most useful
resource has been the <a href="https://prometheus.io/">prometheus.io</a> site documentation.</p>

<p>It took us a while to choose an exporter.  We originally went with
<a href="https://collectd.org/">collectd</a> but ran into limitations with this.  We’re
working on writing an
<a href="https://github.com/CanonicalLtd/prometheus-openstack-exporter">openstack-exporter</a>
now and were a bit surprised to find there is no good, working, example how to
write exporter from scratch.</p>

<p>Some challenges we’ve run into are: No downsampling support, no long term
storage solution (yet), and we were surprised by the default 2 week retention
period. There's currently no tie-in with Juju, but <a href="https://launchpad.net/prometheus-registration">we’re working on it</a>!</p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>Once we got the hang of exporters, we found they were very easy to write and
have given us very useful metrics.  For example we are developing an
openstack-exporter for our cloud environments.  We’ve also seen very quick
cross-team adoption from our DevOps and WebOps groups and developers.  We don’t
yet have alerting in place but expect to see a lot more to come once we get to
this phase of the transition.</p>

<h2 id="what-do-you-think-the-future-holds-for-canonical-and-prometheus?">What do you think the future holds for Canonical and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-canonical-and-prometheus" name="what-do-you-think-the-future-holds-for-canonical-and-prometheus"></a>
</h2>

<p>We expect Prometheus to be a significant part of our monitoring and reporting
infrastructure, providing the metrics gathering and storage for numerous
current and future systems. We see it potentially replacing Nagios as for
alerting.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/10/12/interview-with-justwatch/">Interview with JustWatch</a></h1>
        <aside>Posted at: October 12, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, JustWatch talks
about how they established their monitoring.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-justwatch-does?">Can you tell us about yourself and what JustWatch does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-justwatch-does" name="can-you-tell-us-about-yourself-and-what-justwatch-does"></a>
</h2>

<p>For consumers, <a href="https://www.justwatch.com">JustWatch</a> is a streaming search
engine that helps to find out where to watch movies and TV shows legally online
and in theaters. You can search movie content across all major streaming
providers like Netflix, HBO, Amazon Video, iTunes, Google Play, and many others
in 17 countries.</p>

<p>For our clients like movie studios or Video on Demand providers, we are an
international movie marketing company that collects anonymized data about
purchase behavior and movie taste of fans worldwide from our consumer apps. We
help studios to advertise their content to the right audience and make digital
video advertising a lot more efficient in minimizing waste coverage.</p>

<p><img src="/assets/blog/2016-10-12/JW_logo_long_black-cb56076c127.jpg" alt="JustWatch logo"></p>

<p>Since our launch in 2014 we went from zero to one of the largest 20k websites
internationally without spending a single dollar on marketing - becoming the
largest streaming search engine worldwide in under two years. Currently, with
an engineering team of just 10, we build and operate a fully dockerized stack
of about 50 micro- and macro-services, running mostly on
<a href="https://kubernetes.io">Kubernetes</a>.</p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>At prior companies many of us worked with most of the open-source monitoring
products there are. We have quite some experience working with
<a href="https://www.nagios.org/">Nagios</a>, <a href="https://www.icinga.org/">Icinga</a>,
<a href="http://www.zabbix.com/">Zabbix</a>,
<a href="https://mmonit.com/monit/documentation/">Monit</a>,
<a href="http://munin-monitoring.org/">Munin</a>, <a href="https://graphiteapp.org/">Graphite</a> and
a few other systems. At one company I helped build a distributed Nagios setup
with Puppet. This setup was nice, since new services automatically showed up in
the system, but taking instances out was still painful. As soon as you have
some variance in your systems, the host and service based monitoring suites
just don’t fit quite well. The label-based approach Prometheus took was
something I always wanted to have, but didn’t find before.</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>At JustWatch the public Prometheus announcement hit exactly the right time. We
mostly had blackbox monitoring for the first few months of the company -
<a href="https://aws.amazon.com/cloudwatch/">CloudWatch</a> for some of the most important
internal metrics, combined with a external services like
<a href="https://www.pingdom.com/">Pingdom</a> for detecting site-wide outages. Also, none
of the classical host-based solutions satisfied us. In a world of containers
and microservices, host-based tools like Icinga,
<a href="https://www.thruk.org/">Thruk</a> or Zabbix felt antiquated and not ready for the
job. When we started to investigate whitebox monitoring, some of us luckily
attended the Golang Meetup where Julius and Björn announced Prometheus. We
quickly set up a Prometheus server and started to instrument our Go services
(we use almost only Go for the backend). It was amazing how easy that was - the
design felt like being cloud- and service-oriented as a first principle and
never got in the way.</p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>Transitioning wasn't that hard, as timing wise, we were lucky enough to go from
no relevant monitoring directly to Prometheus.</p>

<p>The transition to Prometheus was mostly including the Go client into our apps
and wrapping the HTTP handlers. We also wrote and deployed several exporters,
including the <a href="https://github.com/prometheus/node_exporter">node_exporter</a> and
several exporters for cloud provider APIs. In our experience monitoring and
alerting is a project that is never finished, but the bulk of the work was done
within a few weeks as a side project.</p>

<p>Since the deployment of Prometheus we tend to look into metrics whenever we
miss something or when we are designing new services from scratch.</p>

<p>It took some time to fully grasp the elegance of PromQL and labels concept
fully, but the effort really paid off.</p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>Prometheus enlightened us by making it incredibly easy to reap the benefits
from whitebox monitoring and label-based canary deployments. The out-of-the-box
metrics for many Golang aspects (HTTP Handler, Go Runtime) helped us to get to
a return on investment very quickly - goroutine metrics alone saved the day
multiple times. The only monitoring component we actually liked before -
<a href="http://grafana.org/">Grafana</a> - feels like a natural fit for Prometheus and
has allowed us to create some very helpful dashboards. We appreciated that
Prometheus didn't try to reinvent the wheel but rather fit in perfectly with
the best solution out there. Another huge improvement on predecessors was
Prometheus's focus on actually getting the math right (percentiles, etc.). In
other systems, we were never quite sure if the operations offered made sense.
Especially percentiles are such a natural and necessary way of reasoning about
microservice performance that it felt great that they get first class
treatment.</p>

<p><img src="/assets/blog/2016-10-12/prometheus-dashboard-db-cbc4db3f322.jpg" alt="Database Dashboard"></p>

<p>The integrated service discovery makes it super easy to manage the scrape
targets. For Kubernetes, everything just works out-of-the-box. For some other
systems not running on Kubernetes yet, we use a
<a href="https://www.consul.io/">Consul-based</a> approach. All it takes to get an
application monitored by Prometheus is to add the client, expose <code>/metrics</code> and
set one simple annotation on the Container/Pod. This low coupling takes out a
lot of friction between development and operations - a lot of services are
built well orchestrated from the beginning, because it's simple and fun.</p>

<p>The combination of time-series and clever functions make for awesome alerting
super-powers. Aggregations that run on the server and treating both
time-series, combinations of them and even functions on those combinations as
first-class citizens makes alerting a breeze - often times after the fact.</p>

<h2 id="what-do-you-think-the-future-holds-for-justwatch-and-prometheus?">What do you think the future holds for JustWatch and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-justwatch-and-prometheus" name="what-do-you-think-the-future-holds-for-justwatch-and-prometheus"></a>
</h2>

<p>While we value very much that Prometheus doesn't focus on being shiny but on
actually working and delivering value while being reasonably easy to deploy and
operate - especially the Alertmanager leaves a lot to be desired yet. Just some
simple improvements like simplified interactive alert building and editing in
the frontend would go a long way in working with alerts being even simpler.</p>

<p>We are really looking forward to the ongoing improvements in the storage layer,
including remote storage. We also hope for some of the approaches taken in
<a href="https://github.com/weaveworks/prism">Project Prism</a> and
<a href="https://github.com/digitalocean/vulcan">Vulcan</a> to be backported to core
Prometheus. The most interesting topics for us right now are GCE Service
Discovery, easier scaling, and much longer retention periods (even at the cost
of colder storage and much longer query times for older events).</p>

<p>We are also looking forward to use Prometheus for more non-technical
departments as well. We’d like to cover most of our KPIs with Prometheus to
allow everyone to create beautiful dashboards, as well as alerts. We're
currently even planning to abuse the awesome alert engine for a new, internal
business project as well - stay tuned!</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/09/21/interview-with-compose/">Interview with Compose</a></h1>
        <aside>Posted at: September 21, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, Compose talks
about their monitoring journey from Graphite and InfluxDB to Prometheus.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-compose-does?">Can you tell us about yourself and what Compose does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-compose-does" name="can-you-tell-us-about-yourself-and-what-compose-does"></a>
</h2>

<p><a href="https://www.compose.com/">Compose</a> delivers production-ready database clusters
as a service to developers around the world. An app developer can come to us
and in a few clicks have a multi-host, highly available, automatically backed
up and secure database ready in minutes. Those database deployments then
autoscale up as demand increases so a developer can spend their time on
building their great apps, not on running their database.</p>

<p>We have tens of clusters of hosts across at least two regions in each of AWS,
Google Cloud Platform and SoftLayer. Each cluster spans availability zones
where supported and is home to around 1000 highly-available database
deployments in their own private networks. More regions and providers are in
the works.</p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>Before Prometheus, a number of different metrics systems were tried. The first
system we tried was <a href="https://graphiteapp.org/">Graphite</a>, which worked pretty
well initially, but the sheer volume of different metrics we had to store,
combined with the way Whisper files are stored and accessed on disk, quickly
overloaded our systems. While we were aware that Graphite could be scaled
horizontally relatively easily, it would have been an expensive cluster.
<a href="https://www.influxdata.com/">InfluxDB</a> looked more promising so we started
trying out the early-ish versions of that and it seemed to work well for a good
while. Goodbye Graphite. </p>

<p>The earlier versions of InfluxDB had some issues with data corruption
occasionally. We semi-regularly had to purge all of our metrics. It wasn’t a
devastating loss for us normally, but it was irritating. The continued promises
of features that never materialised frankly wore on us.</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>It seemed to combine better efficiency with simpler operations than other
options.</p>

<p>Pull-based metric gathering puzzled us at first, but we soon realised the
benefits. Initially it seemed like it could be far too heavyweight to scale
well in our environment where we often have several hundred containers with
their own metrics on each host, but by combining it with Telegraf, we can
arrange to have each host export metrics for all its containers (as well as its
overall resource metrics) via a single Prometheus scrape target.</p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>We are a Chef shop so we spun up a largish instance with a big EBS volume and
then reached right for a <a href="https://github.com/rayrod2030/chef-prometheus">community chef
cookbook</a> for Prometheus.</p>

<p>With Prometheus up on a host, we wrote a small Ruby script that uses the Chef
API to query for all our hosts, and write out a Prometheus target config file.
We use this file with a <code>file_sd_config</code> to ensure all hosts are discovered and
scraped as soon as they register with Chef. Thanks to Prometheus’ open
ecosystem, we were able to use Telegraf out of the box with a simple config to
export host-level metrics directly.</p>

<p>We were testing how far a single Prometheus would scale and waiting for it to
fall over. It didn’t! In fact it handled the load of host-level metrics scraped
every 15 seconds for around 450 hosts across our newer infrastructure with very
little resource usage.</p>

<p>We have a lot of containers on each host so we were expecting to have to start
to shard Prometheus once we added all memory usage metrics from those too, but
Prometheus just kept on going without any drama and still without getting too
close to saturating its resources. We currently monitor over 400,000 distinct
metrics every 15 seconds for around 40,000 containers on 450 hosts with a
single m4.xlarge prometheus instance with 1TB of storage. You can see our host
dashboard for this host below. Disk IO on the 1TB gp2 SSD EBS volume will
probably be the limiting factor eventually. Our initial guess is well
over-provisioned for now, but we are growing fast in both metrics gathered and
hosts/containers to monitor.</p>

<p><img src="/assets/blog/2016-09-21/compose-host-dashboard-cb97623cae4.png" alt="Prometheus Host Dashboard"></p>

<p>At this point the Prometheus server we’d thrown up to test with was vastly more
reliable than the InfluxDB cluster we had doing the same job before, so we did
some basic work to make it less of a single-point-of-failure. We added another
identical node scraping all the same targets, then added a simple failover
scheme with keepalived + DNS updates. This was now more highly available than
our previous system so we switched our customer-facing graphs to use Prometheus
and tore down the old system.</p>

<p><img src="/assets/blog/2016-09-21/compose-memory-stats-cb6c2d95184.png" alt="Prometheus-powered memory metrics for PostgresSQL containers in our app"></p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>Our previous monitoring setup was unreliable and difficult to manage. With
Prometheus we have a system that’s working well for graphing lots of metrics,
and we have team members suddenly excited about new ways to use it rather than
wary of touching the metrics system we used before.</p>

<p>The cluster is simpler too, with just two identical nodes. As we grow, we know
we’ll have to shard the work across more Prometheus hosts and have considered a
few ways to do this.</p>

<h2 id="what-do-you-think-the-future-holds-for-compose-and-prometheus?">What do you think the future holds for Compose and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-compose-and-prometheus" name="what-do-you-think-the-future-holds-for-compose-and-prometheus"></a>
</h2>

<p>Right now we have only replicated the metrics we already gathered in previous
systems - basic memory usage for customer containers as well as host-level
resource usage for our own operations. The next logical step is enabling the
database teams to push metrics to the local Telegraf instance from inside the
DB containers so we can record database-level stats too without increasing
number of targets to scrape.</p>

<p>We also have several other systems that we want to get into Prometheus to get
better visibility. We run our apps on Mesos and have integrated basic Docker
container metrics already, which is better than previously, but we also want to
have more of the infrastructure components in the Mesos cluster recording to
the central Prometheus so we can have centralised dashboards showing all
elements of supporting system health from load balancers right down to app
metrics.</p>

<p>Eventually we will need to shard Prometheus. We already split customer
deployments among many smaller clusters for a variety of reasons so the one
logical option would be to move to a smaller Prometheus server (or a pair for
redundancy) per cluster rather than a single global one.</p>

<p>For most reporting needs this is not a big issue as we usually don’t need
hosts/containers from different clusters in the same dashboard, but we may keep
a small global cluster with much longer retention and just a modest number of
down-sampled and aggregated metrics from each cluster’s Prometheus using
Recording Rules.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/09/14/interview-with-digitalocean/">Interview with DigitalOcean</a></h1>
        <aside>Posted at: September 14, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Next in our series of interviews with users of Prometheus, DigitalOcean talks
about how they use Prometheus. Carlos Amedee also talked about <a href="https://www.youtube.com/watch?v=ieo3lGBHcy8">the social
aspects of the rollout</a> at PromCon
2016.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-digitalocean-does?">Can you tell us about yourself and what DigitalOcean does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-digitalocean-does" name="can-you-tell-us-about-yourself-and-what-digitalocean-does"></a>
</h2>

<p>My name is Ian Hansen and I work on the platform metrics team.
<a href="https://www.digitalocean.com/">DigitalOcean</a> provides simple cloud computing.
To date, we’ve created 20 million Droplets (SSD cloud servers) across 13
regions. We also recently released a new Block Storage product.</p>

<p><img src="/assets/blog/2016-09-14/DO_Logo_Horizontal_Blue-3db19536-cb89e8e1298.png" alt="DigitalOcean logo"></p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>Before Prometheus, we were running <a href="https://graphiteapp.org/">Graphite</a> and
<a href="http://opentsdb.net/">OpenTSDB</a>. Graphite was used for smaller-scale
applications and OpenTSDB was used for collecting metrics from all of our
physical servers via <a href="https://collectd.org/">Collectd</a>.
<a href="https://www.nagios.org/">Nagios</a> would pull these databases to trigger alerts.
We do still use Graphite but we no longer run OpenTSDB.</p>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>I was frustrated with OpenTSDB because I was responsible for keeping the
cluster online, but found it difficult to guard against metric storms.
Sometimes a team would launch a new (very chatty) service that would impact the
total capacity of the cluster and hurt my SLAs. </p>

<p>We are able to blacklist/whitelist new metrics coming in to OpenTSDB, but
didn’t have a great way to guard against chatty services except for
organizational process (which was hard to change/enforce). Other teams were
frustrated with the query language and the visualization tools available at the
time. I was chatting with Julius Volz about push vs pull metric systems and was
sold in wanting to try Prometheus when I saw that I would really be in control
of my SLA when I get to determine what I’m pulling and how frequently. Plus, I
really really liked the query language.</p>

<h2 id="how-did-you-transition?">How did you transition?<a class="header-anchor" href="#how-did-you-transition" name="how-did-you-transition"></a>
</h2>

<p>We were gathering metrics via Collectd sending to OpenTSDB. Installing the
<a href="https://github.com/prometheus/node_exporter">Node Exporter</a> in parallel with
our already running Collectd setup allowed us to start experimenting with
Prometheus. We also created a custom exporter to expose Droplet metrics. Soon,
we had feature parity with our OpenTSDB service and started turning off
Collectd and then turned off the OpenTSDB cluster.</p>

<p>People really liked Prometheus and the visualization tools that came with it.
Suddenly, my small metrics team had a backlog that we couldn’t get to fast
enough to make people happy, and instead of providing and maintaining
Prometheus for people’s services, we looked at creating tooling to make it as
easy as possible for other teams to run their own Prometheus servers and to
also run the common exporters we use at the company.</p>

<p>Some teams have started using Alertmanager, but we still have a concept of
pulling Prometheus from our existing monitoring tools.</p>

<h2 id="what-improvements-have-you-seen-since-switching?">What improvements have you seen since switching?<a class="header-anchor" href="#what-improvements-have-you-seen-since-switching" name="what-improvements-have-you-seen-since-switching"></a>
</h2>

<p>We’ve improved our insights on hypervisor machines. The data we could get out
of Collectd and Node Exporter is about the same, but it’s much easier for our
team of golang developers to create a new custom exporter that exposes data
specific to the services we run on each hypervisor.</p>

<p>We’re exposing better application metrics. It’s easier to learn and teach how
to create a Prometheus metric that can be aggregated correctly later. With
Graphite it’s easy to create a metric that can’t be aggregated in a certain way
later because the dot-separated-name wasn’t structured right.</p>

<p>Creating alerts is much quicker and simpler than what we had before, plus in a
language that is familiar. This has empowered teams to create better alerting
for the services they know and understand because they can iterate quickly.</p>

<h2 id="what-do-you-think-the-future-holds-for-digitalocean-and-prometheus?">What do you think the future holds for DigitalOcean and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-digitalocean-and-prometheus" name="what-do-you-think-the-future-holds-for-digitalocean-and-prometheus"></a>
</h2>

<p>We’re continuing to look at how to make collecting metrics as easy as possible
for teams at DigitalOcean. Right now teams are running their own Prometheus
servers for the things they care about, which allowed us to gain observability
we otherwise wouldn’t have had as quickly. But, not every team should have to
know how to run Prometheus. We’re looking at what we can do to make Prometheus
as automatic as possible so that teams can just concentrate on what queries and
alerts they want on their services and databases.</p>

<p>We also created <a href="https://github.com/digitalocean/vulcan">Vulcan</a> so that we
have long-term data storage, while retaining the Prometheus Query Language that
we have built tooling around and trained people how to use.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/09/07/interview-with-shuttlecloud/">Interview with ShuttleCloud</a></h1>
        <aside>Posted at: September 7, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>Continuing our series of interviews with users of Prometheus, ShuttleCloud talks about how they began using Prometheus. Ignacio from ShuttleCloud also explained how <a href="https://www.youtube.com/watch?v=gMHa4Yh8avk">Prometheus Is Good for Your Small Startup</a> at PromCon 2016.</em></p>

<h2 id="what-does-shuttlecloud-do?">What does ShuttleCloud do?<a class="header-anchor" href="#what-does-shuttlecloud-do" name="what-does-shuttlecloud-do"></a>
</h2>

<p>ShuttleCloud is the world’s most scalable email and contacts data importing system. We help some of the leading email and address book providers, including Google and Comcast, increase user growth and engagement by automating the switching experience through data import. </p>

<p>By integrating our API into their offerings, our customers allow their users to easily migrate their email and contacts from one participating provider to another, reducing the friction users face when switching to a new provider. The 24/7 email providers supported include all major US internet service providers: Comcast, Time Warner Cable, AT&amp;T, Verizon, and more.</p>

<p>By offering end users a simple path for migrating their emails (while keeping complete control over the import tool’s UI), our customers dramatically improve user activation and onboarding.</p>

<p><img src="/assets/blog/2016-09-07/gmail-integration-cbeb0164c27.png" alt="ShuttleCloud's integration with Gmail">
<strong><em>ShuttleCloud’s <a href="https://support.google.com/mail/answer/164640?hl=en">integration</a> with Google’s Gmail Platform.</em></strong> <em>Gmail has imported data for 3 million users with our API.</em></p>

<p>ShuttleCloud’s technology encrypts all the data required to process an import, in addition to following the most secure standards (SSL, oAuth) to ensure the confidentiality and integrity of API requests. Our technology allows us to guarantee our platform’s high availability, with up to 99.5% uptime assurances. </p>

<p><img src="/assets/blog/2016-09-07/shuttlecloud-numbers-cb34514c568.png" alt="ShuttleCloud by Numbers"></p>

<h2 id="what-was-your-pre-prometheus-monitoring-experience?">What was your pre-Prometheus monitoring experience?<a class="header-anchor" href="#what-was-your-pre-prometheus-monitoring-experience" name="what-was-your-pre-prometheus-monitoring-experience"></a>
</h2>

<p>In the beginning, a proper monitoring system for our infrastructure was not one of our main priorities. We didn’t have as many projects and instances as we currently have, so we worked with other simple systems to alert us if anything was not working properly and get it under control.</p>

<ul>
<li>We had a set of automatic scripts to monitor most of the operational metrics for the machines. These were cron-based and executed, using Ansible from a centralized machine. The alerts were emails sent directly to the entire development team.</li>
<li>We trusted Pingdom for external blackbox monitoring and checking that all our frontends were up. They provided an easy interface and alerting system in case any of our external services were not reachable.</li>
</ul>

<p>Fortunately, big customers arrived, and the SLAs started to be more demanding. Therefore, we needed something else to measure how we were performing and to ensure that we were complying with all SLAs. One of the features we required was to have accurate stats about our performance and business metrics (i.e., how many migrations finished correctly), so reporting was more on our minds than monitoring. </p>

<p>We developed the following system:</p>

<p><img src="/assets/blog/2016-09-07/Prometheus-System-1-cba8c7f335c.jpg" alt="Initial Shuttlecloud System"></p>

<ul>
<li><p>The source of all necessary data is a status database in a CouchDB. There, each document represents one status of an operation. This information is processed by the Status Importer and stored in a relational manner in a MySQL database.</p></li>
<li>
<p>A component gathers data from that database, with the information aggregated and post-processed into several views. </p>

<ul>
<li>One of the views is the email report, which we needed for reporting purposes. This is sent via email. </li>
<li>The other view pushes data to a dashboard, where it can be easily controlled. The dashboard service we used was external. We trusted Ducksboard, not only because the dashboards were easy to set up and looked beautiful, but also because they provided automatic alerts if a threshold was reached.</li>
</ul>
</li>
</ul>

<p>With all that in place, it didn’t take us long to realize that we would need a proper metrics, monitoring, and alerting system as the number of projects started to increase. </p>

<p>Some drawbacks of the systems we had at that time were:</p>

<ul>
<li>No centralized monitoring system. Each metric type had a different one:

<ul>
<li>System metrics → Scripts run by Ansible.</li>
<li>Business metrics → Ducksboard and email reports.</li>
<li>Blackbox metrics → Pingdom.</li>
</ul>
</li>
<li>No standard alerting system. Each metric type had different alerts (email, push notification, and so on).</li>
<li>Some business metrics had no alerts. These were reviewed manually.</li>
</ul>

<h2 id="why-did-you-decide-to-look-at-prometheus?">Why did you decide to look at Prometheus?<a class="header-anchor" href="#why-did-you-decide-to-look-at-prometheus" name="why-did-you-decide-to-look-at-prometheus"></a>
</h2>

<p>We analyzed several monitoring and alerting systems. We were eager to get our hands dirty and check if the a solution would succeed or fail. The system we decided to put to the test was Prometheus, for the following reasons:</p>

<ul>
<li>First of all, you don’t have to define a fixed metric system to start working with it; metrics can be added or changed in the future. This provides valuable flexibility when you don’t know all of the metrics you want to monitor yet.</li>
<li>If you know anything about Prometheus, you know that metrics can have labels that abstract us from the fact that different time series are considered. This, together with its query language, provided even more flexibility and a powerful tool. For example, we can have the same metric defined for different environments or projects and get a specific time series or aggregate certain metrics with the appropriate labels:

<ul>
<li>
<code>http_requests_total{job="my_super_app_1",environment="staging"}</code> - the time series corresponding to the staging environment for the app "my_super_app_1".</li>
<li>
<code>http_requests_total{job="my_super_app_1"}</code> - the time series for all environments for the app "my_super_app_1".</li>
<li>
<code>http_requests_total{environment="staging"}</code> - the time series for all staging environments for all jobs.</li>
</ul>
</li>
<li>Prometheus supports a DNS service for service discovery. We happened to already  have an internal DNS service.</li>
<li>There is no need to install any external services (unlike Sensu, for example, which needs a data-storage service like Redis and a message bus like RabbitMQ). This might not be a deal breaker, but it definitely makes the test easier to perform, deploy, and maintain.</li>
<li>Prometheus is quite easy to install, as you only need to download an executable Go file. The Docker container also works well and it is easy to start.</li>
</ul>

<h2 id="how-do-you-use-prometheus?">How do you use Prometheus?<a class="header-anchor" href="#how-do-you-use-prometheus" name="how-do-you-use-prometheus"></a>
</h2>

<p>Initially we were only using some metrics provided out of the box by the <a href="https://github.com/prometheus/node_exporter">node_exporter</a>, including:</p>

<ul>
<li>hard drive usage.</li>
<li>memory usage.</li>
<li>if an instance is up or down.</li>
</ul>

<p>Our internal DNS service is integrated to be used for service discovery, so every new instance is automatically monitored.</p>

<p>Some of the metrics we used, which were not provided by the node_exporter by default, were exported using the <a href="https://github.com/prometheus/node_exporter#textfile-collector">node_exporter textfile collector</a> feature. The first alerts we declared on the Prometheus Alertmanager were mainly related to the operational metrics mentioned above.</p>

<p>We later developed an operation exporter that allowed us to know the status of the system almost in real time. It exposed business metrics, namely the statuses of all operations, the number of incoming migrations, the number of finished migrations, and the number of errors. We could aggregate these on the Prometheus side and let it calculate different rates. </p>

<p>We decided to export and monitor the following metrics:</p>

<ul>
<li><code>operation_requests_total</code></li>
<li><code>operation_statuses_total</code></li>
<li><code>operation_errors_total</code></li>
</ul>

<p><img src="/assets/blog/2016-09-07/Prometheus-System-2-cbee2d089c8.jpg" alt="Shuttlecloud Prometheus System"></p>

<p>We have most of our services duplicated in two Google Cloud Platform availability zones. That includes the monitoring system. It’s straightforward to have more than one operation exporter in two or more different zones, as Prometheus can aggregate the data from all of them and make one metric (i.e., the maximum of all). We currently don’t have Prometheus or the Alertmanager in HA — only a metamonitoring instance — but we are working on it.</p>

<p>For external blackbox monitoring, we use the Prometheus <a href="https://github.com/prometheus/blackbox_exporter">Blackbox Exporter</a>. Apart from checking if our external frontends are up, it is especially useful for having metrics for SSL certificates’ expiration dates. It even checks the whole chain of certificates. Kudos to Robust Perception for explaining it perfectly in their <a href="https://www.robustperception.io/get-alerted-before-your-ssl-certificates-expire/">blogpost</a>.</p>

<p>We set up some charts in Grafana for visual monitoring in some dashboards, and the integration with Prometheus was trivial. The query language used to define the charts is the same as in Prometheus, which simplified their creation a lot.</p>

<p>We also integrated Prometheus with Pagerduty and created a schedule of people on-call for the critical alerts. For those alerts that were not considered critical, we only sent an email.</p>

<h2 id="how-does-prometheus-make-things-better-for-you?">How does Prometheus make things better for you?<a class="header-anchor" href="#how-does-prometheus-make-things-better-for-you" name="how-does-prometheus-make-things-better-for-you"></a>
</h2>

<p>We can't compare Prometheus with our previous solution because we didn’t have one, but we can talk about what features of Prometheus are highlights for us:</p>

<ul>
<li>It has very few maintenance requirements.</li>
<li>It’s efficient: one machine can handle monitoring the whole cluster.</li>
<li>The community is friendly—both dev and users. Moreover, <a href="https://www.robustperception.io/blog/">Brian’s blog</a> is a very good resource.</li>
<li>It has no third-party requirements; it’s just the server and the exporters. (No RabbitMQ or Redis needs to be maintained.)</li>
<li>Deployment of Go applications is a breeze.</li>
</ul>

<h2 id="what-do-you-think-the-future-holds-for-shuttlecloud-and-prometheus?">What do you think the future holds for ShuttleCloud and Prometheus?<a class="header-anchor" href="#what-do-you-think-the-future-holds-for-shuttlecloud-and-prometheus" name="what-do-you-think-the-future-holds-for-shuttlecloud-and-prometheus"></a>
</h2>

<p>We’re very happy with Prometheus, but new exporters are always welcome (Celery or Spark, for example). </p>

<p>One question that we face every time we add a new alarm is: how do we test that the alarm works as expected? It would be nice to have a way to inject fake metrics in order to raise an alarm, to test it.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/09/04/promcon-2016-its-a-wrap/">PromCon 2016 - It's a wrap!</a></h1>
        <aside>Posted at: September 4, 2016 by Julius Volz</aside>
        <article class="doc-content">
          <h2 id="what-happened">What happened<a class="header-anchor" href="#what-happened" name="what-happened"></a>
</h2>

<p>Last week, eighty Prometheus users and developers from around the world came
together for two days in Berlin for the first-ever conference about the
Prometheus monitoring system: <a href="https://promcon.io/">PromCon 2016</a>. The goal of
this conference was to exchange knowledge, best practices, and experience
gained using Prometheus. We also wanted to grow the community and help people
build professional connections around service monitoring. Here are some
impressions from the first morning:</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/09/04/promcon-2016-its-a-wrap/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/07/23/pull-does-not-scale-or-does-it/">Pull doesn't scale - or does it?</a></h1>
        <aside>Posted at: July 23, 2016 by Julius Volz</aside>
        <article class="doc-content">
          <p>Let's talk about a particularly persistent myth. Whenever there is a discussion
about monitoring systems and Prometheus's pull-based metrics collection
approach comes up, someone inevitably chimes in about how a pull-based approach
just “fundamentally doesn't scale”. The given reasons are often vague or only
apply to systems that are fundamentally different from Prometheus. In fact,
having worked with pull-based monitoring at the largest scales, this claim runs
counter to our own operational experience.</p>

<p>We already have an FAQ entry about
<a href="/docs/introduction/faq/#why-do-you-pull-rather-than-push?">why Prometheus chooses pull over push</a>,
but it does not focus specifically on scaling aspects. Let's have a closer look
at the usual misconceptions around this claim and analyze whether and how they
would apply to Prometheus.</p>

<h2 id="prometheus-is-not-nagios">Prometheus is not Nagios<a class="header-anchor" href="#prometheus-is-not-nagios" name="prometheus-is-not-nagios"></a>
</h2>

<p>When people think of a monitoring system that actively pulls, they often think
of Nagios. Nagios has a reputation of not scaling well, in part due to spawning
subprocesses for active checks that can run arbitrary actions on the Nagios
host in order to determine the health of a certain host or service. This sort
of check architecture indeed does not scale well, as the central Nagios host
quickly gets overwhelmed. As a result, people usually configure checks to only
be executed every couple of minutes, or they run into more serious problems.</p>

<p>However, Prometheus takes a fundamentally different approach altogether.
Instead of executing check scripts, it only collects time series data from a
set of instrumented targets over the network. For each target, the Prometheus
server simply fetches the current state of all metrics of that target over HTTP
(in a highly parallel way, using goroutines) and has no other execution
overhead that would be pull-related. This brings us to the next point:</p>

<h2 id="it-doesn't-matter-who-initiates-the-connection">It doesn't matter who initiates the connection<a class="header-anchor" href="#it-doesn-t-matter-who-initiates-the-connection" name="it-doesn-t-matter-who-initiates-the-connection"></a>
</h2>

<p>For scaling purposes, it doesn't matter who initiates the TCP connection over
which metrics are then transferred. Either way you do it, the effort for
establishing a connection is small compared to the metrics payload and other
required work.</p>

<p>But a push-based approach could use UDP and avoid connection establishment
altogether, you say! True, but the TCP/HTTP overhead in Prometheus is still
negligible compared to the other work that the Prometheus server has to do to
ingest data (especially persisting time series data on disk). To put some
numbers behind this: a single big Prometheus server can easily store millions
of time series, with a record of 800,000 incoming samples per second (as
measured with real production metrics data at SoundCloud). Given a 10-seconds
scrape interval and 700 time series per host, this allows you to monitor over
10,000 machines from a single Prometheus server. The scaling bottleneck here
has never been related to pulling metrics, but usually to the speed at which
the Prometheus server can ingest the data into memory and then sustainably
persist and expire data on disk/SSD.</p>

<p>Also, although networks are pretty reliable these days, using a TCP-based pull
approach makes sure that metrics data arrives reliably, or that the monitoring
system at least knows immediately when the metrics transfer fails due to a
broken network.</p>

<h2 id="prometheus-is-not-an-event-based-system">Prometheus is not an event-based system<a class="header-anchor" href="#prometheus-is-not-an-event-based-system" name="prometheus-is-not-an-event-based-system"></a>
</h2>

<p>Some monitoring systems are event-based. That is, they report each individual
event (an HTTP request, an exception, you name it) to a central monitoring
system immediately as it happens. This central system then either aggregates
the events into metrics (StatsD is the prime example of this) or stores events
individually for later processing (the ELK stack is an example of that). In
such a system, pulling would be problematic indeed: the instrumented service
would have to buffer events between pulls, and the pulls would have to happen
incredibly frequently in order to simulate the same “liveness” of the
push-based approach and not overwhelm event buffers.</p>

<p>However, again, Prometheus is not an event-based monitoring system. You do not
send raw events to Prometheus, nor can it store them. Prometheus is in the
business of collecting aggregated time series data. That means that it's only
interested in regularly collecting the current <em>state</em> of a given set of
metrics, not the underlying events that led to the generation of those metrics.
For example, an instrumented service would not send a message about each HTTP
request to Prometheus as it is handled, but would simply count up those
requests in memory.  This can happen hundreds of thousands of times per second
without causing any monitoring traffic. Prometheus then simply asks the service
instance every 15 or 30 seconds (or whatever you configure) about the current
counter value and stores that value together with the scrape timestamp as a
sample. Other metric types, such as gauges, histograms, and summaries, are
handled similarly. The resulting monitoring traffic is low, and the pull-based
approach also does not create problems in this case.</p>

<h2 id="but-now-my-monitoring-needs-to-know-about-my-service-instances!">But now my monitoring needs to know about my service instances!<a class="header-anchor" href="#but-now-my-monitoring-needs-to-know-about-my-service-instances" name="but-now-my-monitoring-needs-to-know-about-my-service-instances"></a>
</h2>

<p>With a pull-based approach, your monitoring system needs to know which service
instances exist and how to connect to them. Some people are worried about the
extra configuration this requires on the part of the monitoring system and see
this as an operational scalability problem.</p>

<p>We would argue that you cannot escape this configuration effort for
serious monitoring setups in any case: if your monitoring system doesn't know
what the world <em>should</em> look like and which monitored service instances
<em>should</em> be there, how would it be able to tell when an instance just never
reports in, is down due to an outage, or really is no longer meant to exist?
This is only acceptable if you never care about the health of individual
instances at all, like when you only run ephemeral workers where it is
sufficient for a large-enough number of them to report in some result. Most
environments are not exclusively like that.</p>

<p>If the monitoring system needs to know the desired state of the world anyway,
then a push-based approach actually requires <em>more</em> configuration in total. Not
only does your monitoring system need to know what service instances should
exist, but your service instances now also need to know how to reach your
monitoring system. A pull approach not only requires less configuration,
it also makes your monitoring setup more flexible. With pull, you can just run
a copy of production monitoring on your laptop to experiment with it. It also
allows you just fetch metrics with some other tool or inspect metrics endpoints
manually. To get high availability, pull allows you to just run two identically
configured Prometheus servers in parallel. And lastly, if you have to move the
endpoint under which your monitoring is reachable, a pull approach does not
require you to reconfigure all of your metrics sources.</p>

<p>On a practical front, Prometheus makes it easy to configure the desired state
of the world with its built-in support for a wide variety of service discovery
mechanisms for cloud providers and container-scheduling systems: Consul,
Marathon, Kubernetes, EC2, DNS-based SD, Azure, Zookeeper Serversets, and more.
Prometheus also allows you to plug in your own custom mechanism if needed.
In a microservice world or any multi-tiered architecture, it is also
fundamentally an advantage if your monitoring system uses the same method to
discover targets to monitor as your service instances use to discover their
backends. This way you can be sure that you are monitoring the same targets
that are serving production traffic and you have only one discovery mechanism
to maintain.</p>

<h2 id="accidentally-ddos-ing-your-monitoring">Accidentally DDoS-ing your monitoring<a class="header-anchor" href="#accidentally-ddos-ing-your-monitoring" name="accidentally-ddos-ing-your-monitoring"></a>
</h2>

<p>Whether you pull or push, any time-series database will fall over if you send
it more samples than it can handle. However, in our experience it's slightly
more likely for a push-based approach to accidentally bring down your
monitoring. If the control over what metrics get ingested from which instances
is not centralized (in your monitoring system), then you run into the danger of
experimental or rogue jobs suddenly pushing lots of garbage data into your
production monitoring and bringing it down.  There are still plenty of ways how
this can happen with a pull-based approach (which only controls where to pull
metrics from, but not the size and nature of the metrics payloads), but the
risk is lower. More importantly, such incidents can be mitigated at a central
point.</p>

<h2 id="real-world-proof">Real-world proof<a class="header-anchor" href="#real-world-proof" name="real-world-proof"></a>
</h2>

<p>Besides the fact that Prometheus is already being used to monitor very large
setups in the real world (like using it to <a href="http://promcon.io/talks/scaling_to_a_million_machines_with_prometheus/">monitor millions of machines at
DigitalOcean</a>),
there are other prominent examples of pull-based monitoring being used
successfully in the largest possible environments. Prometheus was inspired by
Google's Borgmon, which was (and partially still is) used within Google to
monitor all its critical production services using a pull-based approach. Any
scaling issues we encountered with Borgmon at Google were not due its pull
approach either. If a pull-based approach scales to a global environment with
many tens of datacenters and millions of machines, you can hardly say that pull
doesn't scale.</p>

<h2 id="but-there-are-other-problems-with-pull!">But there are other problems with pull!<a class="header-anchor" href="#but-there-are-other-problems-with-pull" name="but-there-are-other-problems-with-pull"></a>
</h2>

<p>There are indeed setups that are hard to monitor with a pull-based approach.
A prominent example is when you have many endpoints scattered around the
world which are not directly reachable due to firewalls or complicated
networking setups, and where it's infeasible to run a Prometheus server
directly in each of the network segments. This is not quite the environment for
which Prometheus was built, although workarounds are often possible (<a href="/docs/practices/pushing/">via the
Pushgateway or restructuring your setup</a>). In any
case, these remaining concerns about pull-based monitoring are usually not
scaling-related, but due to network operation difficulties around opening TCP
connections.</p>

<h2 id="all-good-then?">All good then?<a class="header-anchor" href="#all-good-then" name="all-good-then"></a>
</h2>

<p>This article addresses the most common scalability concerns around a pull-based
monitoring approach. With Prometheus and other pull-based systems being used
successfully in very large environments and the pull aspect not posing a
bottleneck in reality, the result should be clear: the “pull doesn't scale”
argument is not a real concern. We hope that future debates will focus on
aspects that matter more than this red herring.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/07/18/prometheus-1-0-released/">Prometheus reaches 1.0</a></h1>
        <aside>Posted at: July 18, 2016 by Fabian Reinartz on behalf of the Prometheus team</aside>
        <article class="doc-content">
          <p>In January, we published a blog post on <a href="https://prometheus.io/blog/2016/01/26/one-year-of-open-prometheus-development/">Prometheus’s first year of public existence</a>, summarizing what has been an amazing journey for us, and hopefully an innovative and useful monitoring solution for you.
Since then, <a href="https://prometheus.io/blog/2016/05/09/prometheus-to-join-the-cloud-native-computing-foundation/">Prometheus has also joined the Cloud Native Computing Foundation</a>, where we are in good company, as the second charter project after <a href="http://kubernetes.io/">Kubernetes</a>.</p>

<p>Our recent work has focused on delivering a stable API and user interface, marked by version 1.0 of Prometheus.
We’re thrilled to announce that we’ve reached this goal, and <a href="https://github.com/prometheus/prometheus/releases/tag/v1.0.0">Prometheus 1.0 is available today</a>.</p>

<h2 id="what-does-1.0-mean-for-you?">What does 1.0 mean for you?<a class="header-anchor" href="#what-does-1-0-mean-for-you" name="what-does-1-0-mean-for-you"></a>
</h2>

<p>If you have been using Prometheus for a while, you may have noticed that the rate and impact of breaking changes significantly decreased over the past year.
In the same spirit, reaching 1.0 means that subsequent 1.x releases will remain API stable. Upgrades won’t break programs built atop the Prometheus API, and updates won’t require storage re-initialization or deployment changes. Custom dashboards and alerts will remain intact across 1.x version updates as well.
We’re confident Prometheus 1.0 is a solid monitoring solution. Now that the Prometheus server has reached a stable API state, other modules will follow it to their own stable version 1.0 releases over time.</p>

<h3 id="fine-print">Fine print<a class="header-anchor" href="#fine-print" name="fine-print"></a>
</h3>

<p>So what does API stability mean? Prometheus has a large surface area and some parts are certainly more mature than others.
There are two simple categories, <em>stable</em> and <em>unstable</em>:</p>

<p>Stable as of v1.0 and throughout the 1.x series:</p>

<ul>
<li>The query language and data model</li>
<li>Alerting and recording rules</li>
<li>The ingestion exposition formats</li>
<li>Configuration flag names</li>
<li>HTTP API (used by dashboards and UIs)</li>
<li>Configuration file format (minus the non-stable service discovery integrations, see below)</li>
<li>Alerting integration with Alertmanager 0.1+ for the foreseeable future</li>
<li>Console template syntax and semantics</li>
</ul>

<p>Unstable and may change within 1.x:</p>

<ul>
<li>The remote storage integrations (InfluxDB, OpenTSDB, Graphite) are still experimental and will at some point be removed in favor of a generic, more sophisticated API that allows storing samples in arbitrary storage systems.</li>
<li>Several service discovery integrations are new and need to keep up with fast evolving systems. Hence, integrations with Kubernetes, Marathon, Azure, and EC2 remain in beta status and are subject to change. However, changes will be clearly announced.</li>
<li>Exact flag meanings may change as necessary. However, changes will never cause the server to not start with previous flag configurations.</li>
<li>Go APIs of packages that are part of the server.</li>
<li>HTML generated by the web UI.</li>
<li>The metrics in the <code>/metrics</code> endpoint of Prometheus itself.</li>
<li>Exact on-disk format. Potential changes however, will be forward compatible and transparently handled by Prometheus.</li>
</ul>

<h2 id="so-prometheus-is-complete-now?">So Prometheus is complete now?<a class="header-anchor" href="#so-prometheus-is-complete-now" name="so-prometheus-is-complete-now"></a>
</h2>

<p>Absolutely not. We have a long roadmap ahead of us, full of great features to implement. Prometheus will not stay in 1.x for years to come. The infrastructure space is evolving rapidly and we fully intend for Prometheus to evolve with it.
This means that we will remain willing to question what we did in the past and are open to leave behind things that have lost relevance. There will be new major versions of Prometheus to facilitate future plans like persistent long-term storage, newer iterations of Alertmanager, internal storage improvements, and many things we don’t even know about yet.</p>

<h2 id="closing-thoughts">Closing thoughts<a class="header-anchor" href="#closing-thoughts" name="closing-thoughts"></a>
</h2>

<p>We want to thank our fantastic community for field testing new versions, filing bug reports, contributing code, helping out other community members, and shaping Prometheus by participating in countless productive discussions.
In the end, you are the ones who make Prometheus successful.</p>

<p>Thank you, and keep up the great work!</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/05/09/prometheus-to-join-the-cloud-native-computing-foundation/">Prometheus to Join the Cloud Native Computing Foundation</a></h1>
        <aside>Posted at: May 9, 2016 by Julius Volz on behalf of the Prometheus core developers</aside>
        <article class="doc-content">
          <p>Since the inception of Prometheus, we have been looking for a sustainable
governance model for the project that is independent of any single company.
Recently, we have been in discussions with the newly formed <a href="https://cncf.io/">Cloud Native
Computing Foundation</a> (CNCF), which is backed by Google,
CoreOS, Docker, Weaveworks, Mesosphere, and <a href="https://cncf.io/about/members">other leading infrastructure
companies</a>.</p>

<p>Today, we are excited to announce that the CNCF's Technical Oversight Committee
<a href="http://lists.cncf.io/pipermail/cncf-toc/2016-May/000198.html">voted unanimously</a> to
accept Prometheus as a second hosted project after Kubernetes! You can find
more information about these plans in the
<a href="https://cncf.io/news/news/2016/05/cloud-native-computing-foundation-accepts-prometheus-second-hosted-project">official press release by the CNCF</a>.</p>

<p>By joining the CNCF, we hope to establish a clear and sustainable project
governance model, as well as benefit from the resources, infrastructure, and
advice that the independent foundation provides to its members.</p>

<p>We think that the CNCF and Prometheus are an ideal thematic match, as both
focus on bringing about a modern vision of the cloud.</p>

<p>In the following months, we will be working with the CNCF on finalizing the
project governance structure. We will report back when there are more details
to announce.</p>

        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/05/08/when-to-use-varbit-chunks/">When (not) to use varbit chunks</a></h1>
        <aside>Posted at: May 8, 2016 by Björn “Beorn” Rabenstein</aside>
        <article class="doc-content">
          <p>The embedded time serie database (TSDB) of the Prometheus server organizes the
raw sample data of each time series in chunks of constant 1024 bytes size. In
addition to the raw sample data, a chunk contains some meta-data, which allows
the selection of a different encoding for each chunk. The most fundamental
distinction is the encoding version. You select the version for newly created
chunks via the command line flag <code>-storage.local.chunk-encoding-version</code>. Up to
now, there were only two supported versions: 0 for the original delta encoding,
and 1 for the improved double-delta encoding. With release
<a href="https://github.com/prometheus/prometheus/releases/tag/0.18.0">0.18.0</a>, we
added version 2, which is another variety of double-delta encoding. We call it
<em>varbit encoding</em> because it involves a variable bit-width per sample within
the chunk. While version 1 is superior to version 0 in almost every aspect,
there is a real trade-off between version 1 and 2. This blog post will help you
to make that decision. Version 1 remains the default encoding, so if you want
to try out version 2 after reading this article, you have to select it
explicitly via the command line flag. There is no harm in switching back and
forth, but note that existing chunks will not change their encoding version
once they have been created. However, these chunks will gradually be phased out
according to the configured retention time and will thus be replaced by chunks
with the encoding specified in the command-line flag.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/05/08/when-to-use-varbit-chunks/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/05/01/interview-with-showmax/">Interview with ShowMax</a></h1>
        <aside>Posted at: May 1, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>This is the second in a series of interviews with users of Prometheus, allowing
them to share their experiences of evaluating and using Prometheus.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-showmax-does?">Can you tell us about yourself and what ShowMax does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-showmax-does" name="can-you-tell-us-about-yourself-and-what-showmax-does"></a>
</h2>

<p>I’m Antonin Kral, and I’m leading research and architecture for
<a href="http://www.showmax.com">ShowMax</a>. Before that, I’ve held architectural and CTO
roles for the past 12 years.</p>

<p>ShowMax is a subscription video on demand service that launched in South Africa
in 2015. We’ve got an extensive content catalogue with more than 20,000
episodes of TV shows and movies. Our service is currently available in 65
countries worldwide. While better known rivals are skirmishing in America and
Europe, ShowMax is battling a more difficult problem: how do you binge-watch
in a barely connected village in sub-Saharan Africa? Already 35% of video
around the world is streamed, but there are still so many places the revolution
has left untouched.</p>

<p><img src="/assets/blog/2016-05-01/showmax-logo-cb5f41c49fe.png" alt="ShowMax logo"></p>

<p>We are managing about 50 services running mostly on private clusters built
around CoreOS. They are primarily handling API requests from our clients
(Android, iOS, AppleTV, JavaScript, Samsung TV, LG TV etc), while some of them
are used internally. One of the biggest internal pipelines is video encoding
which can occupy 400+ physical servers when handling large ingestion batches.</p>

<p>The majority of our back-end services are written in Ruby, Go or Python. We use
EventMachine when writing apps in Ruby (Goliath on MRI, Puma on JRuby). Go is
typically used in apps that require large throughput and don’t have so much
business logic. We’re very happy with Falcon for services written in Python.
Data is stored in PostgreSQL and ElasticSearch clusters. We use etcd and custom
tooling for configuring Varnishes for routing requests.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/05/01/interview-with-showmax/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/03/23/interview-with-life360/">Interview with Life360</a></h1>
        <aside>Posted at: March 23, 2016 by Brian Brazil</aside>
        <article class="doc-content">
          <p><em>This is the first in a series of interviews with users of Prometheus, allowing
them to share their experiences of evaluating and using Prometheus. Our first
interview is with Daniel from Life360.</em></p>

<h2 id="can-you-tell-us-about-yourself-and-what-life360-does?">Can you tell us about yourself and what Life360 does?<a class="header-anchor" href="#can-you-tell-us-about-yourself-and-what-life360-does" name="can-you-tell-us-about-yourself-and-what-life360-does"></a>
</h2>

<p>I’m Daniel Ben Yosef, a.k.a, dby, and I’m an Infrastructure Engineer for
<a href="https://www.life360.com/">Life360</a>, and before that, I’ve held systems
engineering roles for the past 9 years.</p>

<p>Life360 creates technology that helps families stay connected, we’re the Family
Network app for families. We’re quite busy handling these families - at peak
we serve 700k requests per minute for 70 million registered families.</p>

<p><a href="https://www.life360.com/"><img src="/assets/blog/2016-03-23/life360_horizontal_logo_gradient_rgb-cbd4306cf57.png" style="width: 444px; height:177px"></a></p>

<p>We manage around 20 services in production, mostly handling location requests
from mobile clients (Android, iOS, and Windows Phone), spanning over 150+
instances at peak. Redundancy and high-availability are our goals and we strive
to maintain 100% uptime whenever possible because families trust us to be
available.</p>

<p>We hold user data in both our MySQL multi-master cluster and in our 12-node
Cassandra ring which holds around 4TB of data at any given time. We have
services written in Go, Python, PHP, as well as plans to introduce Java to our
stack. We use Consul for service discovery, and of course our Prometheus setup
is integrated with it.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/03/23/interview-with-life360/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/03/03/custom-alertmanager-templates/">Custom Alertmanager Templates</a></h1>
        <aside>Posted at: March 3, 2016 by Fabian Reinartz</aside>
        <article class="doc-content">
          <p>The Alertmanager handles alerts sent by Prometheus servers and sends
notifications about them to different receivers based on their labels.</p>

<p>A receiver can be one of many different integrations such as PagerDuty, Slack,
email, or a custom integration via the generic webhook interface (for example <a href="https://github.com/fabxc/jiralerts">JIRA</a>).</p>

<h2 id="templates">Templates<a class="header-anchor" href="#templates" name="templates"></a>
</h2>

<p>The messages sent to receivers are constructed via templates.
Alertmanager comes with default templates but also allows defining custom
ones.</p>

<p>In this blog post, we will walk through a simple customization of Slack
notifications.</p>

<p>We use this simple Alertmanager configuration that sends all alerts to Slack:</p>

<pre><code class="yaml">global:
  slack_api_url: '&lt;slack_webhook_url&gt;'

route:
  receiver: 'slack-notifications'
  # All alerts in a notification have the same value for these labels.
  group_by: [alertname, datacenter, app]

receivers:
- name: 'slack-notifications'
  slack_configs:
  - channel: '#alerts'
</code></pre>

<p>By default, a Slack message sent by Alertmanager looks like this:</p>

<p><img src="/assets/blog/2016-03-03/slack_alert_before-cb51e526b7f.png" alt=""></p>

<p>It shows us that there is one firing alert, followed by the label values of
the alert grouping (alertname, datacenter, app) and further label values the
alerts have in common (critical).</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/03/03/custom-alertmanager-templates/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2016/01/26/one-year-of-open-prometheus-development/">One Year of Open Prometheus Development</a></h1>
        <aside>Posted at: January 26, 2016 by Julius Volz</aside>
        <article class="doc-content">
          <h2 id="the-beginning">The beginning<a class="header-anchor" href="#the-beginning" name="the-beginning"></a>
</h2>

<p>A year ago today, we officially announced Prometheus to the wider world. This
is a great opportunity for us to look back and share some of the wonderful
things that have happened to the project since then. But first, let's start at
the beginning.</p>

<p>Although we had already started Prometheus as an open-source project on GitHub in
2012, we didn't make noise about it at first. We wanted to give the project
time to mature and be able to experiment without friction. Prometheus was
gradually introduced for production monitoring at
<a href="https://soundcloud.com/">SoundCloud</a> in 2013 and then saw more and more
usage within the company, as well as some early adoption by our friends at
Docker and Boexever in 2014. Over the years, Prometheus was growing more and
more mature and although it was already solving people's monitoring problems,
it was still unknown to the wider public.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2016/01/26/one-year-of-open-prometheus-development/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2015/08/17/service-discovery-with-etcd/">Custom service discovery with etcd</a></h1>
        <aside>Posted at: August 17, 2015 by Fabian Reinartz</aside>
        <article class="doc-content">
          <p>In a <a href="/blog/2015/06/01/advanced-service-discovery/">previous post</a> we
introduced numerous new ways of doing service discovery in Prometheus.
Since then a lot has happened. We improved the internal implementation and
received fantastic contributions from our community, adding support for
service discovery with Kubernetes and Marathon. They will become available
with the release of version 0.16.</p>

<p>We also touched on the topic of <a href="/blog/2015/06/01/advanced-service-discovery/#custom-service-discovery">custom service discovery</a>.</p>

<p>Not every type of service discovery is generic enough to be directly included
in Prometheus. Chances are your organisation has a proprietary
system in place and you just have to make it work with Prometheus.
This does not mean that you cannot enjoy the benefits of automatically
discovering new monitoring targets.</p>

<p>In this post we will implement a small utility program that connects a custom
service discovery approach based on <a href="https://coreos.com/etcd/">etcd</a>, the
highly consistent distributed key-value store, to Prometheus.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2015/08/17/service-discovery-with-etcd/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2015/06/24/monitoring-dreamhack/">Monitoring DreamHack - the World's Largest Digital Festival</a></h1>
        <aside>Posted at: June 24, 2015 by Christian Svensson (DreamHack Network Team)</aside>
        <article class="doc-content">
          <p><em>Editor's note: This article is a guest post written by a Prometheus user.</em></p>

<p><strong>If you are operating the network for 10,000's of demanding gamers, you need to
really know what is going on inside your network. Oh, and everything needs to be
built from scratch in just five days.</strong></p>

<p>If you have never heard about <a href="http://www.dreamhack.se/">DreamHack</a> before, here
is the pitch: Bring 20,000 people together and have the majority of them bring
their own computer.  Mix in professional gaming (eSports), programming contests,
and live music concerts. The result is the world's largest festival dedicated
solely to everything digital.</p>

<p>To make such an event possible, there needs to be a lot of infrastructure in
place. Ordinary infrastructures of this size take months to build, but the crew
at DreamHack builds everything from scratch in just five days. This of course
includes stuff like configuring network switches, but also building the
electricity distribution, setting up stores for food and drinks, and even
building the actual tables.</p>

<p>The team that builds and operates everything related to the network is
officially called the Network team, but we usually refer to ourselves as <em>tech</em>
or <em>dhtech</em>. This post is going to focus on the work of dhtech and how we used
Prometheus during DreamHack Summer 2015 to try to kick our monitoring up another
notch.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2015/06/24/monitoring-dreamhack/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2015/06/18/practical-anomaly-detection/">Practical Anomaly Detection</a></h1>
        <aside>Posted at: June 18, 2015 by Brian Brazil</aside>
        <article class="doc-content">
          <p>In his <em><a href="http://www.kitchensoap.com/2015/05/01/openlettertomonitoringproducts/">Open Letter To Monitoring/Metrics/Alerting Companies</a></em>,
John Allspaw asserts that attempting "to detect anomalies perfectly, at the right time, is not possible".</p>

<p>I have seen several attempts by talented engineers to build systems to
automatically detect and diagnose problems based on time series data. While it
is certainly possible to get a demonstration working, the data always turned
out to be too noisy to make this approach work for anything but the simplest of
real-world systems.</p>

<p>All hope is not lost though. There are many common anomalies which you can
detect and handle with custom-built rules. The Prometheus <a href="../../../../../docs/querying/basics/">query
language</a> gives you the tools to discover
these anomalies while avoiding false positives.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2015/06/18/practical-anomaly-detection/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2015/06/01/advanced-service-discovery/">Advanced Service Discovery in Prometheus 0.14.0</a></h1>
        <aside>Posted at: June 1, 2015 by Fabian Reinartz, Julius Volz</aside>
        <article class="doc-content">
          <p>This week we released Prometheus v0.14.0 — a version with many long-awaited additions
and improvements.</p>

<p>On the user side, Prometheus now supports new service discovery mechanisms. In
addition to DNS-SRV records, it now supports <a href="https://www.consul.io">Consul</a>
out of the box, and a file-based interface allows you to connect your own
discovery mechanisms. Over time, we plan to add other common service discovery
mechanisms to Prometheus.</p>

<p>Aside from many smaller fixes and improvements, you can now also reload your configuration during
runtime by sending a <code>SIGHUP</code> to the Prometheus process. For a full list of changes, check the
<a href="https://github.com/prometheus/prometheus/blob/master/CHANGELOG.md#0140--2015-06-01">changelog for this release</a>.</p>

<p>In this blog post, we will take a closer look at the built-in service discovery mechanisms and provide
some practical examples. As an additional resource, see
<a href="/docs/operating/configuration">Prometheus's configuration documentation</a>.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2015/06/01/advanced-service-discovery/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
      <div class="blog doc-content">
        <h1><a href="/blog/2015/04/24/prometheus-monitring-spreads-through-the-internet/">Prometheus Monitoring Spreads through the Internet</a></h1>
        <aside>Posted at: April 24, 2015 by Brian Brazil</aside>
        <article class="doc-content">
          <p>It has been almost three months since we publicly announced Prometheus version
0.10.0, and we're now at version 0.13.1.</p>

<p><a href="https://developers.soundcloud.com/blog/prometheus-monitoring-at-soundcloud">SoundCloud's announcement blog post</a>
remains the best overview of the key components of Prometheus, but there has
been a lot of other online activity around Prometheus. This post will let you
catch up on anything you missed.</p>

<p>In the future, we will use this blog to publish more articles and announcements
to help you get the most out of Prometheus.</p>

<div class='read-more'><a class='btn btn-primary' href='/blog/2015/04/24/prometheus-monitring-spreads-through-the-internet/'>Continue reading &raquo;</a></div>
        </article>
      </div>
    
  </div>
  <div class="col-md-3 side-nav-col">
  <ul class="nav navbar-nav side-nav">
    <li>
      <span class="nav-header">Blog posts</span>
      <ul class="nav active">
      
        <li><a href="/blog/2016/11/16/interview-with-canonical/">Interview with Canonical</a></li>
      
        <li><a href="/blog/2016/10/12/interview-with-justwatch/">Interview with JustWatch</a></li>
      
        <li><a href="/blog/2016/09/21/interview-with-compose/">Interview with Compose</a></li>
      
        <li><a href="/blog/2016/09/14/interview-with-digitalocean/">Interview with DigitalOcean</a></li>
      
        <li><a href="/blog/2016/09/07/interview-with-shuttlecloud/">Interview with ShuttleCloud</a></li>
      
        <li><a href="/blog/2016/09/04/promcon-2016-its-a-wrap/">PromCon 2016 - It's a wrap!</a></li>
      
        <li><a href="/blog/2016/07/23/pull-does-not-scale-or-does-it/">Pull doesn't scale - or does it?</a></li>
      
        <li><a href="/blog/2016/07/18/prometheus-1-0-released/">Prometheus reaches 1.0</a></li>
      
        <li><a href="/blog/2016/05/09/prometheus-to-join-the-cloud-native-computing-foundation/">Prometheus to Join the Cloud Native Computing Foundation</a></li>
      
        <li><a href="/blog/2016/05/08/when-to-use-varbit-chunks/">When (not) to use varbit chunks</a></li>
      
        <li><a href="/blog/2016/05/01/interview-with-showmax/">Interview with ShowMax</a></li>
      
        <li><a href="/blog/2016/03/23/interview-with-life360/">Interview with Life360</a></li>
      
        <li><a href="/blog/2016/03/03/custom-alertmanager-templates/">Custom Alertmanager Templates</a></li>
      
        <li><a href="/blog/2016/01/26/one-year-of-open-prometheus-development/">One Year of Open Prometheus Development</a></li>
      
        <li><a href="/blog/2015/08/17/service-discovery-with-etcd/">Custom service discovery with etcd</a></li>
      
        <li><a href="/blog/2015/06/24/monitoring-dreamhack/">Monitoring DreamHack - the World's Largest Digital Festival</a></li>
      
        <li><a href="/blog/2015/06/18/practical-anomaly-detection/">Practical Anomaly Detection</a></li>
      
        <li><a href="/blog/2015/06/01/advanced-service-discovery/">Advanced Service Discovery in Prometheus 0.14.0</a></li>
      
        <li><a href="/blog/2015/04/24/prometheus-monitring-spreads-through-the-internet/">Prometheus Monitoring Spreads through the Internet</a></li>
      
      </ul>
    </li>
  </ul>
</div>

</div>

  <hr>

<footer>
  <p class="pull-left">
    &copy; Prometheus Authors 2016
  </p>
</footer>

</div>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-2.2.2.min.js" integrity="sha256-36cp2Co+/62rEAAYHLmRCPIych47CvdM+uTBJwSzWjI=" crossorigin="anonymous"></script>
    <script src="/assets/bootstrap-3.3.1/js/bootstrap.min-cb2616d3564.js"></script>
    <script src="/assets/docs-cb53fb1bfd3.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="/assets/ie10-viewport-bug-workaround-cbb5a0dd7ce.js"></script>
    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-58468480-1', 'auto');
      ga('send', 'pageview');
    </script>
  </body>
</html>

